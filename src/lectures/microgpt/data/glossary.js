const GLOSSARY = [
  { term: "Autograd", def: {tr:"Hesaplama grafı üzerinde otomatik türev alma sistemi. Her operasyonun yerel türevini bilerek chain rule ile geri yayılım yapar.",en:"Automatic differentiation system on computation graphs. Performs backpropagation via chain rule knowing each operation's local derivative."}, cat: "temel", week: 2 },
  { term: "Autoregressive", def: {tr:"Her adımda kendi çıktısını girdi olarak kullanan üretim yöntemi. GPT autoregressive: önceki tokenlara bakarak sonraki tokeni tahmin eder.",en:"Generation method that uses its own output as input at each step. GPT is autoregressive: predicts next token by looking at previous tokens."}, cat: "model", week: 0 },
  { term: "Attention", def: {tr:"Her tokenın diğer tokenlara dinamik ağırlıklarla 'dikkat etme' mekanizması. Formül: softmax(Q·Kᵀ/√d)·V",en:"Mechanism for each token to 'attend' to others with dynamic weights. Formula: softmax(Q·Kᵀ/√d)·V"}, cat: "mimari", week: 3 },
  { term: "Backward Pass", def: {tr:"Loss'tan parametrelere doğru gradient hesaplama süreci. Chain rule ile her düğümün gradientini hesaplar.",en:"Process of computing gradients from loss to parameters. Computes each node's gradient via chain rule."}, cat: "temel", week: 2 },
  { term: "BOS/EOS", def: {tr:"Beginning/End of Sequence. Dizinin başını ve sonunu işaret eden özel token.",en:"Beginning/End of Sequence. Special token marking start and end of a sequence."}, cat: "veri", week: 1 },
  { term: "Causal Mask", def: {tr:"Her tokenın sadece önceki tokenlara bakabilmesini sağlayan maskeleme. GPT'nin 'kopya çekmesini' engeller.",en:"Masking that ensures each token can only look at previous tokens. Prevents GPT from 'cheating'."}, cat: "mimari", week: 3 },
  { term: "Chain Rule", def: {tr:"Bileşik fonksiyonların türev kuralı: f(g(x))' = f'(g(x)) × g'(x). Autograd'ın temelindeki matematik.",en:"Derivative rule for composite functions: f(g(x))' = f'(g(x)) × g'(x). The math behind autograd."}, cat: "temel", week: 2 },
  { term: "Cross-Entropy Loss", def: {tr:"L = -log(P(doğru_token)). Modelin tahmin kalitesini ölçen kayıp fonksiyonu. Düşük loss = iyi model.",en:"L = -log(P(correct_token)). Loss function measuring prediction quality. Low loss = good model."}, cat: "eğitim", week: 5 },
  { term: "Dot Product", def: {tr:"İki vektörün element-wise çarpımlarının toplamı: a·b = Σ aᵢbᵢ. Benzerlik ölçüsü olarak kullanılır.",en:"Sum of element-wise products of two vectors: a·b = Σ aᵢbᵢ. Used as a similarity measure."}, cat: "temel", week: 3 },
  { term: "Embedding", def: {tr:"Token ID'yi çok boyutlu sürekli vektöre dönüştüren öğrenilebilir tablo. Bu kodda 28×16 matris.",en:"Learnable lookup table converting token IDs to multi-dimensional continuous vectors. In this code: 28×16 matrix."}, cat: "mimari", week: 1 },
  { term: "Forward Pass", def: {tr:"Girdi → model katmanları → çıktı (logits) hesaplama süreci. İleri yönde data akışı.",en:"Process of computing input → model layers → output (logits). Forward data flow."}, cat: "temel", week: 0 },
  { term: "Loss", def: {tr:"Modelin tahmin kalitesini ölçen hata fonksiyonu. Düşük loss = iyi tahmin. microGPT başlangıç: 3.33, eğitim sonrası: ~2.0.",en:"Error function measuring prediction quality. Low loss = good prediction. microGPT start: 3.33, after training: ~2.0."}, cat: "temel", week: 0 },
  { term: "Parametre", def: {tr:"Modelin öğrenilebilir sayısal değerleri (ağırlıklar). microGPT: 3,648 parametre, GPT-4: ~1T+.",en:"Model's learnable numerical values (weights). microGPT: 3,648 parameters, GPT-4: ~1T+."}, cat: "temel", week: 0 },
  { term: "Hyperparametre", def: {tr:"Eğitim öncesi sabitlenen tasarım kararları: n_embd, n_layer, learning_rate vb. Eğitimle değişmez.",en:"Design decisions fixed before training: n_embd, n_layer, learning_rate etc. Not changed during training."}, cat: "temel", week: 0 },
  { term: "Pipeline", def: {tr:"Veriyi adım adım işleyen sıralı süreç. GPT: tokenize → embed → attend → MLP → predict.",en:"Sequential process that processes data step by step. GPT: tokenize → embed → attend → MLP → predict."}, cat: "temel", week: 0 },
  { term: "Gradient", def: {tr:"Tüm kısmi türevlerin vektörü: ∇f = [∂f/∂w₁, ∂f/∂w₂, ...]. Parametrelerin güncelleme yönünü gösterir.",en:"Vector of all partial derivatives: ∇f = [∂f/∂w₁, ∂f/∂w₂, ...]. Shows parameter update direction."}, cat: "temel", week: 2 },
  { term: "Gradient Descent", def: {tr:"Parametreleri gradient'in ters yönünde güncelleyerek loss'u minimize etme: w -= lr × ∂L/∂w",en:"Minimizing loss by updating parameters in the opposite direction of gradient: w -= lr × ∂L/∂w"}, cat: "eğitim", week: 5 },
  { term: "KV Cache", def: {tr:"Önceki pozisyonların Key ve Value vektörlerini saklayarak tekrar hesaplamayı önleyen optimizasyon.",en:"Optimization that stores previous positions' Key and Value vectors to avoid recomputation."}, cat: "mimari", week: 6 },
  { term: "Learning Rate", def: {tr:"Parametre güncelleme adım boyutu. Çok büyük → patlama, çok küçük → yavaş öğrenme.",en:"Parameter update step size. Too large → explosion, too small → slow learning."}, cat: "eğitim", week: 5 },
  { term: "Logits", def: {tr:"Modelin son katman ham çıktı skorları. Softmax'tan geçirilmeden önceki değerler.",en:"Model's raw output scores from the last layer. Values before softmax."}, cat: "model", week: 1 },
  { term: "MLP (Feed-Forward)", def: {tr:"Her tokena bağımsız uygulanan genişlet→aktive et→daralt ağı. Bu kodda: 16→64→16.",en:"Expand→activate→compress network applied independently to each token. In this code: 16→64→16."}, cat: "mimari", week: 4 },
  { term: "Multi-Head Attention", def: {tr:"Embedding'i birden fazla head'e bölüp her birinde bağımsız attention hesaplama. Farklı kalıplar öğrenir.",en:"Splitting embedding into multiple heads with independent attention computation. Learns different patterns."}, cat: "mimari", week: 3 },
  { term: "ReLU²", def: {tr:"Aktivasyon fonksiyonu: max(0,x)². Negatifler sıfır olur, pozitifler karesel büyür → sparse temsil.",en:"Activation function: max(0,x)². Negatives become zero, positives grow quadratically → sparse representation."}, cat: "mimari", week: 4 },
  { term: "Residual Connection", def: {tr:"x = f(x) + x_skip. Girdiyi çıktıya ekleyerek gradient akışına kestirme yol sağlar.",en:"x = f(x) + x_skip. Provides gradient shortcut by adding input to output."}, cat: "mimari", week: 4 },
  { term: "RMSNorm", def: {tr:"x / √(mean(x²) + ε). LayerNorm'un hızlı versiyonu — ortalama çıkarma adımı yok.",en:"x / √(mean(x²) + ε). Fast version of LayerNorm — no mean subtraction step."}, cat: "mimari", week: 4 },
  { term: "Sampling", def: {tr:"Olasılık dağılımından rastgele token seçme. Temperature ile kontrol edilir.",en:"Randomly selecting a token from probability distribution. Controlled by temperature."}, cat: "model", week: 6 },
  { term: "Softmax", def: {tr:"Ham skorları olasılık dağılımına çevirir: P(i) = exp(xᵢ)/Σexp(xⱼ). Toplam her zaman 1.",en:"Converts raw scores to probability distribution: P(i) = exp(xᵢ)/Σexp(xⱼ). Sum always equals 1."}, cat: "temel", week: 1 },
  { term: "Temperature", def: {tr:"Softmax'ın sivriliğini kontrol eden parametre. T<1 → deterministik, T>1 → rastgele.",en:"Parameter controlling softmax sharpness. T<1 → deterministic, T>1 → random."}, cat: "model", week: 6 },
  { term: "Token", def: {tr:"Modelin işlediği en küçük birim. Bu kodda her karakter (a-z + BOS) bir token.",en:"Smallest unit the model processes. In this code each character (a-z + BOS) is a token."}, cat: "veri", week: 1 },
  { term: "Topological Sort", def: {tr:"DAG'da düğümleri bağımlılık sırasına dizen algoritma. Backward pass'te doğru gradient sırasını sağlar.",en:"Algorithm that orders DAG nodes by dependency. Ensures correct gradient order in backward pass."}, cat: "temel", week: 2 },
  { term: "Transformer", def: {tr:"Attention + MLP + Norm + Residual'dan oluşan mimari. 2017'de tanıtıldı, tüm modern LLM'lerin temeli.",en:"Architecture composed of Attention + MLP + Norm + Residual. Introduced in 2017, foundation of all modern LLMs."}, cat: "mimari", week: 4 },
  { term: "Weight Tying", def: {tr:"Giriş embedding matrisi (wte) ile çıkış projeksiyon matrisinin paylaşılması. Parametre tasarrufu sağlar.",en:"Sharing input embedding matrix (wte) with output projection matrix. Saves parameters."}, cat: "mimari", week: 1 },
  { term: "Scaling Laws", def: {tr:"Model boyutu, veri ve hesaplama artınca loss'un güç yasasıyla düştüğünü gösteren ampirik yasalar (Kaplanick 2020).",en:"Empirical laws showing loss decreases as a power law with model size, data, and compute (Kaplan 2020)."}, cat: "evrim", week: 7 },
  { term: "BPE", def: {tr:"Byte Pair Encoding: En sık karakter çiftlerini birleştirerek alt-kelime token'ları oluşturan tokenization algoritması.",en:"Byte Pair Encoding: Tokenization algorithm creating subword tokens by merging the most frequent character pairs."}, cat: "evrim", week: 7 },
  { term: "RLHF", def: {tr:"Reinforcement Learning from Human Feedback: İnsan tercihleri ile modeli 'iyi davranışa' hizalama yöntemi.",en:"Reinforcement Learning from Human Feedback: Method for aligning the model to 'good behavior' using human preferences."}, cat: "evrim", week: 7 },
  { term: "SFT", def: {tr:"Supervised Fine-Tuning: İnsan yazımı soru-cevap çiftleri ile modeli assistant formatına dönüştürme.",en:"Supervised Fine-Tuning: Converting the model to assistant format using human-written Q&A pairs."}, cat: "evrim", week: 7 },
  { term: "MoE", def: {tr:"Mixture of Experts: Birden fazla uzman ağ, her token sadece birkaçını aktive eder → verimli büyük model.",en:"Mixture of Experts: Multiple expert networks, each token activates only a few → efficient large model."}, cat: "evrim", week: 7 },
  { term: "Flash Attention", def: {tr:"IO-aware tiling ile standart attention'ı 2-4× hızlandıran algoritma. Matematik aynı, bellek erişimi farklı.",en:"Algorithm that speeds up standard attention 2-4× via IO-aware tiling. Same math, different memory access."}, cat: "evrim", week: 7 },
  { term: "RAG", def: {tr:"Retrieval-Augmented Generation: Dış bilgi tabanından ilgili dokümanları çekip yanıta ekleyen yöntem.",en:"Retrieval-Augmented Generation: Method that retrieves relevant documents from external knowledge base and adds to response."}, cat: "evrim", week: 7 },
  { term: "Ablation Study", def: {tr:"Her bileşeni tek tek çıkararak bireysel katkısını ölçen deneysel yöntem. YL araştırmanın temel aracı.",en:"Experimental method measuring individual contribution by removing each component. A fundamental research tool."}, cat: "araştırma", week: 8 },
  { term: "Hessian", def: {tr:"İkinci türev matrisi. Loss landscape'ın eğriliğini gösterir. Newton yöntemi Hessian kullanır.",en:"Second derivative matrix. Shows the curvature of the loss landscape. Newton's method uses the Hessian."}, cat: "araştırma", week: 8 },
  { term: "Isotropy", def: {tr:"Embedding vektörlerinin uzayda eşit dağılımı. Anisotropik = dar koniye sıkışmış = kötü.",en:"Equal distribution of embedding vectors in space. Anisotropic = squeezed into narrow cone = bad."}, cat: "araştırma", week: 8 },
  { term: "Head Pruning", def: {tr:"Gereksiz attention head'lerini kaldırma. Taylor expansion ile importance skoru hesaplanır.",en:"Removing unnecessary attention heads. Importance score computed via Taylor expansion."}, cat: "araştırma", week: 8 },
  { term: "Entropy", def: {tr:"H(X) = -Σp(x)log(p(x)). Belirsizlik ölçüsü. Tokenizer değerlendirmede kullanılır.",en:"H(X) = -Σp(x)log(p(x)). Measure of uncertainty. Used in tokenizer evaluation."}, cat: "araştırma", week: 8 },
  { term: "Pareto Front", def: {tr:"Çok amaçlı optimizasyonda optimal noktalar kümesi. Birini iyileştirmeden diğeri kötüleşmez.",en:"Set of optimal points in multi-objective optimization. Can't improve one without worsening another."}, cat: "araştırma", week: 9 },
  { term: "Knowledge Distillation", def: {tr:"Büyük teacher modelin bilgisini küçük student modele aktarma. Soft targets ile sınıflar arası ilişki aktarılır.",en:"Transferring knowledge from large teacher model to small student model. Inter-class relationships transferred via soft targets."}, cat: "araştırma", week: 9 },
  { term: "Grokking", def: {tr:"Eğitim loss≈0 olduktan çok sonra test loss'un aniden düşmesi. Gecikmeli genelleme fenomeni.",en:"Test loss suddenly dropping long after training loss≈0. A delayed generalization phenomenon."}, cat: "araştırma", week: 9 },
  { term: "RoPE", def: {tr:"Rotary Position Embedding. Q,K vektörlerini pozisyona göre döndürerek göreceli pozisyon bilgisi sağlar.",en:"Rotary Position Embedding. Provides relative position info by rotating Q,K vectors based on position."}, cat: "araştırma", week: 9 },
  { term: "NAS", def: {tr:"Neural Architecture Search. Otomatik mimari arama: arama uzayı + strateji (random/Bayesian) + değerlendirme.",en:"Neural Architecture Search. Automatic architecture search: search space + strategy (random/Bayesian) + evaluation."}, cat: "araştırma", week: 9 },
];


export { GLOSSARY };
