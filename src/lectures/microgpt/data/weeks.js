// â”€â”€â”€ Lecture Content â€” All Weeks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const WEEKS = [
  {
    id: "intro", week: 0, title: { tr: "GiriÅŸ & CanlÄ± Demo", en: "Introduction & Live Demo" }, icon: "ğŸš€", color: "#0EA5E9",
    subtitle: { tr: "microGPT nedir, neden sÄ±fÄ±rdan, kurulum, Ã§alÄ±ÅŸtÄ±rma, sonuÃ§larÄ± gÃ¶zlemleme", en: "What is microGPT, why from scratch, setup, running, observing results" },
    sections: [
      {
        title: { tr: "Derse HoÅŸ Geldiniz â€” Ne Ã–ÄŸreneceksiniz?", en: "Welcome â€” What You Will Learn" },
        viz: "coursePipeline",
        content: "243 satÄ±r saf Python ile tam bir GPT â€” satÄ±r satÄ±r anlayacaksÄ±nÄ±z. HiÃ§bir dÄ±ÅŸ kÃ¼tÃ¼phane yok. YukarÄ±daki pipeline'Ä±n her aÅŸamasÄ±nÄ± ayrÄ± bir hafta iÅŸleyeceÄŸiz.",
        highlight: "\"This file is the complete algorithm. Everything else is just efficiency.\" â€” Andrej Karpathy"
      },
      {
        title: { tr: "Ã–n Bilgi: Yapay Sinir AÄŸÄ± Nedir?", en: "Background: What is a Neural Network?" },
        viz: "neuralNetBasics",
        content: "Sinir aÄŸÄ± = Ã¶ÄŸrenilebilir parametreli bir fonksiyon. 3 sekmeyi keÅŸfedin: ğŸ”¬ NÃ¶ron'da kaydÄ±rÄ±cÄ±larla aÄŸÄ±rlÄ±klarÄ± deÄŸiÅŸtirip Ã§Ä±ktÄ±yÄ± canlÄ± gÃ¶rÃ¼n. ğŸŒŠ Veri AkÄ±ÅŸÄ±'nda verinin nÃ¶rondan nasÄ±l geÃ§tiÄŸini adÄ±m adÄ±m izleyin. ğŸ¯ Mini EÄŸitim'de modelin ev fiyatÄ±nÄ± tahmin etmeyi nasÄ±l Ã¶ÄŸrendiÄŸini simÃ¼le edin.",
        highlight: "EÄŸitim = bu kaydÄ±rÄ±cÄ±larÄ± (wâ‚, wâ‚‚, b) veriye gÃ¶re OTOMATÄ°K ayarlama. GPT'de 3,648 tane var!"
      },
      {
        title: { tr: "Ã–n Bilgi: Dil Modeli Nedir?", en: "Background: What is a Language Model?" },
        viz: "langModelConcept",
        content: "Dil modeli = 'sonraki token ne olabilir?' sorusuna cevap veren olasÄ±lÄ±k makinesi. 3 sekmeyi keÅŸfedin: ğŸ² Autoregressive Ãœretim'de 'emma' isminin harf harf nasÄ±l Ã¼retildiÄŸini izleyin. ğŸ“± Telefon Analojisi'nde harf yazarak autocomplete'in GPT ile aynÄ± mantÄ±kta Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n. ğŸ“š 'emma' EÄŸitimi'nde her harf Ã§iftinin olasÄ±lÄ±ÄŸÄ±nÄ± eÄŸitim Ã¶ncesi/sonrasÄ± karÅŸÄ±laÅŸtÄ±rÄ±n.",
        highlight: "GPT = telefon autocompletenin Ã§ok bÃ¼yÃ¼k Ã¶lÃ§ekli hali. AynÄ± mantÄ±k: Ã¶nceki tokenlara bakarak sonrakini tahmin et."
      },
      {
        title: { tr: "Ne YapÄ±yor Bu Kod? â€” CanlÄ± Pipeline", en: "What Does This Code Do? â€” Live Pipeline" },
        viz: "livePipeline",
        content: "YukarÄ±daki animasyonda â–¶ butonuna basÄ±n â€” 'emma' isminin 9 aÅŸamalÄ± yolculuÄŸunu adÄ±m adÄ±m izleyin. Her kutuya tÄ±klayarak o aÅŸamanÄ±n detayÄ±nÄ± gÃ¶rebilirsiniz.",
        code: `# Tek komutla Ã§alÄ±ÅŸtÄ±rÄ±n:
$ python3 microgpt.py
# vocab size: 28, num params: 3648
# step 1/1000 | loss 3.33 â† rastgele
# step 1000   | loss 2.00 â† Ã¶ÄŸrendi!
# sample 0: kamrin â† yeni, gerÃ§ekÃ§i isim!`,
        highlight: "Her kutuya tÄ±klayarak o aÅŸamanÄ±n detayÄ±nÄ± gÃ¶rebilirsiniz"
      },
      {
        title: { tr: "Neden SÄ±fÄ±rdan? Framework KarÅŸÄ±laÅŸtÄ±rma", en: "Why From Scratch? Framework Comparison" },
        viz: "frameworkCompare",
        content: "YukarÄ±daki panelde 'ArkasÄ±nda ne var?' butonuna tÄ±klayÄ±n â€” PyTorch'un 3 satÄ±rÄ±nÄ±n arkasÄ±nda neler gizlendiÄŸini gÃ¶rÃ¼n. microgpt.py'de aynÄ± iÅŸlem aÃ§Ä±kÃ§a yazÄ±lmÄ±ÅŸ.",
        highlight: "Framework = verimlilik aracÄ±, algoritma deÄŸil. Ã–nce algoritmayÄ± anlayÄ±n, sonra framework hÄ±zlandÄ±rsÄ±n."
      },
      {
        title: { tr: "Ã–n KoÅŸullar & Kurulum", en: "Prerequisites & Setup" },
        content: "Tek gereksinim: Python 3.6+. pip install gerekmez â€” sadece os, math, random kullanÄ±lÄ±r. Kurulum istemiyorsanÄ±z Google Colab'da tarayÄ±cÄ±dan Ã§alÄ±ÅŸtÄ±rabilirsiniz!",
        code: `# Python kontrol:
$ python3 --version  # 3.6+ yeterli

# pip install GEREKMÄ°YOR!
import os      # dosya kontrolÃ¼
import math    # log, exp
import random  # rastgele sayÄ±lar`,
        highlight: "pip install gerekmez â€” sadece os, math, random kullanÄ±lÄ±r"
      },
      {
        title: { tr: "Kodu Ä°ndirme & Ä°lk Ã‡alÄ±ÅŸtÄ±rma", en: "Download & First Run" },
        content: "GitHub Gist'ten tek dosya indirin ve Ã§alÄ±ÅŸtÄ±rÄ±n. Ya da Google Colab'da sÄ±fÄ±r kurulumla baÅŸlayÄ±n â†’ Kaynaklar bÃ¶lÃ¼mÃ¼ndeki ğŸŸ  Colab linkine tÄ±klayÄ±n! Loss dÃ¼ÅŸÃ¼yorsa her ÅŸey doÄŸru!",
        code: `# Ä°ndir:
$ curl -o microgpt.py https://gist.githubusercontent.com/karpathy/.../microgpt.py

# Ã‡alÄ±ÅŸtÄ±r (hÄ±zlÄ± test):
$ python3 microgpt.py --num_steps 200

# Loss dÃ¼ÅŸÃ¼yorsa â†’ model Ã¶ÄŸreniyor âœ“`,
        highlight: "Loss dÃ¼ÅŸÃ¼yorsa her ÅŸey doÄŸru!"
      },
      {
        title: { tr: "7 Temel Parametre â€” Ä°nteraktif Lab", en: "7 Core Parameters â€” Interactive Lab" },
        viz: "paramDist",
        content: "AÅŸaÄŸÄ±daki kaydÄ±rÄ±cÄ±larla parametreleri deÄŸiÅŸtirin â€” parametre sayÄ±sÄ±, bellek ve komut satÄ±rÄ± komutu canlÄ± gÃ¼ncellenir. n_embd artÄ±nca parametreler KARESEL bÃ¼yÃ¼r!",
        code: `# VarsayÄ±lan:
$ python3 microgpt.py  # 3,648 param

# BÃ¼yÃ¼k model:
$ python3 microgpt.py --n_embd 32 --n_layer 2
# â†’ ~14K param (4Ã— artÄ±ÅŸ!)`,
        highlight: "n_embd artÄ±nca parametreler KARESEL bÃ¼yÃ¼r!"
      },
      {
        title: { tr: "Kendi Verinizi Kullanma", en: "Using Your Own Data" },
        content: "input.txt'i deÄŸiÅŸtirerek TÃ¼rkÃ§e isimler, ÅŸehir adlarÄ± veya hayvan isimleri Ã¶ÄŸretebilirsiniz. Vocab otomatik hesaplanÄ±r.",
        code: `# TÃ¼rkÃ§e isim verisi:
$ cat > input.txt << EOF
ahmet
mehmet
ayse
fatma
zeynep
EOF
$ python3 microgpt.py --num_steps 500
# SonuÃ§: mehet, aysun, fatem...`,
        highlight: "Vocab otomatik hesaplanÄ±r"
      },
      {
        title: { tr: "Ãœretim Evrimi â€” CanlÄ± SimÃ¼lasyon", en: "Generation Evolution â€” Live Simulation" },
        viz: "trainingEvolution",
        content: "â–¶ BaÅŸlat'a basÄ±n â€” eÄŸitim boyunca loss'un dÃ¼ÅŸÃ¼ÅŸÃ¼nÃ¼ ve Ã¼retilen isimlerin kalite artÄ±ÅŸÄ±nÄ± canlÄ± izleyin. KaydÄ±rÄ±cÄ± ile istediÄŸiniz aÅŸamaya zÄ±playabilirsiniz.",
        highlight: "AdÄ±m 1: 'xqwpzml' (rastgele) â†’ AdÄ±m 1000: 'ellora' (gerÃ§ekÃ§i). AynÄ± 3,648 parametre â€” sadece eÄŸitim!"
      },
      {
        title: { tr: "GPT Ailesi â€” Ã–lÃ§ek Kulesi", en: "GPT Family â€” Scale Tower" },
        viz: "gptScaleTower",
        content: "Kulelerin Ã¼zerine gelin â€” her modelin detaylarÄ±nÄ± gÃ¶rÃ¼n. microGPT bir bisiklet, GPT-4 bir uzay mekiÄŸi â€” ama aynÄ± fizik kurallarÄ± geÃ§erli.",
        highlight: "Bu kodu anlarsanÄ±z GPT-4'Ã¼n %90'Ä±nÄ± anlarsÄ±nÄ±z. Kalan %10: RoPE, GQA, SwiGLU, MoE."
      }
    ]
  },

  {
    id: "tokenization", week: 1, title: { tr: "Tokenization & Embedding", en: "Tokenization & Embedding" }, icon: "ğŸ”¤", color: "#8B5CF6",
    subtitle: { tr: "Metni sayÄ±lara, sayÄ±larÄ± vektÃ¶rlere Ã§evirme â€” modelin dÃ¼nyayÄ± gÃ¶rme biÃ§imi", en: "Converting text to numbers, numbers to vectors â€” how the model sees the world" },
    sections: [
      {
        title: { tr: "Ã–n Bilgi: Bilgisayarlar Metni NasÄ±l Ä°ÅŸler?", en: "Background: How Do Computers Process Text?" },
        viz: "tokenFlow",
        content: "Bilgisayarlar metin iÅŸleyemez â€” her ÅŸey sayÄ±sal olmalÄ±dÄ±r. Tokenization = metni modelin anlayacaÄŸÄ± ID dizisine Ã§evirme. Bu kodda her karakter = bir token.",
        highlight: "Tokenization = modelin 'gÃ¶zlÃ¼ÄŸÃ¼'. FarklÄ± tokenizer = farklÄ± dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼."
      },
      {
        title: { tr: "Temel TanÄ±mlar â€” Token, Vocab, Logit", en: "Key Definitions â€” Token, Vocab, Logit" },
        viz: "neuralNetBasics",
        content: "Token = modelin iÅŸlediÄŸi en kÃ¼Ã§Ã¼k birim (bu kodda karakter). Vocabulary = tÃ¼m token kÃ¼mesi (a-z + BOS = 27). Logit = modelin ham Ã§Ä±ktÄ± skorlarÄ± (softmax Ã¶ncesi).",
        highlight: "Metin â†’ sayÄ± â†’ vektÃ¶r dÃ¶nÃ¼ÅŸÃ¼mÃ¼ olmadan model hiÃ§bir ÅŸey yapamaz."
      },
      {
        title: { tr: "ğŸ® Tokenizer Oyun AlanÄ± â€” CanlÄ± SimÃ¼lasyon", en: "ğŸ® Tokenizer Playground â€” Live Simulation" },
        viz: "tokenizerPlayground",
        content: "YukarÄ±ya bir isim yazÄ±n ve â–¶ butonuna basÄ±n â€” tokenization'Ä±n 4 adÄ±mÄ±nÄ± canlÄ± izleyin: karakterlere ayrÄ±lma â†’ ID'lere Ã§evrilme â†’ BOS eklenmesi â†’ eÄŸitim Ã§iftlerinin oluÅŸmasÄ±.",
        code: `# Vocabulary oluÅŸturma:
uchars = sorted(set(''.join(docs)))  # ['a'...'z']
BOS = len(uchars)  # = 26 (Ã¶zel token)

# Tokenize etme:
tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]
# "emma" â†’ [26, 4, 12, 12, 0, 26]`,
        highlight: "YukarÄ±ya bir isim yazÄ±n ve â–¶ butonuna basÄ±n â€” tokenization'Ä±n 4 adÄ±mÄ±nÄ± canlÄ± iz"
      },
      {
        title: { tr: "Ã–n Bilgi: VektÃ¶r Nedir? Neden KullanÄ±rÄ±z?", en: "Background: What is a Vector? Why Use Them?" },
        viz: "vectorConcept",
        content: "VektÃ¶r = sÄ±ralÄ± sayÄ±lar listesi. Benzer ÅŸeyler â†’ yakÄ±n vektÃ¶rler. Bu kodda her token 16 boyutlu bir vektÃ¶rle temsil edilir.",
        highlight: "Embedding = token'Ä± vektÃ¶re Ã§evirme. EÄŸitim sonunda benzer tokenlar yakÄ±n vektÃ¶rlere sahip olur."
      },
      {
        title: { tr: "Token Embedding: ID â†’ VektÃ¶r DÃ¶nÃ¼ÅŸÃ¼mÃ¼", en: "Token Embedding: ID â†’ Vector Transform" },
        viz: "embeddingFlow",
        content: "Embedding tablosu [27Ã—16]. Ä°ÅŸlem basit tablo arama: wte[token_id] â†’ 16-boyutlu vektÃ¶r. BaÅŸlangÄ±Ã§ta rastgele, eÄŸitimle anlam kazanÄ±r.",
        code: `tok_emb = state_dict['wte'][token_id]
# 'a' â†’ ID=0 â†’ wte[0] = [0.02, -0.01, ...]
# EÄŸitim sonrasÄ±: sesli harfler yakÄ±n vektÃ¶rlerde`,
        highlight: "BaÅŸlangÄ±Ã§ta rastgele, eÄŸitimle anlam kazanÄ±r"
      },
      {
        title: { tr: "Position Embedding: SÄ±ra Bilgisi Ekleme", en: "Position Embedding: Adding Order Information" },
        viz: "embeddingFlow",
        content: "Transformer sÄ±ra bilgisi Ä°Ã‡ERMEZ! Position embedding ile her konuma (0-7) Ã¶zgÃ¼ vektÃ¶r eklenir: x = tok_emb + pos_emb.",
        code: `pos_emb = state_dict['wpe'][pos_id]
x = [t + p for t, p in zip(tok_emb, pos_emb)]
# AynÄ± 'a' â†’ pos 0 ve pos 3'te FARKLI temsil`,
        highlight: "Transformer sÄ±ra bilgisi Ä°Ã‡ERMEZ! Position embedding ile her konuma (0-7) Ã¶zgÃ¼ v"
      },
      {
        title: { tr: "Ã–n Bilgi: Matris Ã‡arpÄ±mÄ± (Linear Transform)", en: "Background: Matrix Multiplication (Linear Transform)" },
        viz: "matrixMul",
        content: "Her Ã§Ä±ktÄ± = girdi vektÃ¶rÃ¼ ile aÄŸÄ±rlÄ±k satÄ±rÄ±nÄ±n dot product'Ä±. Transformer'da HER projeksiyon (Wq, Wk, Wv, fc1, fc2) bir matris Ã§arpÄ±mÄ±.",
        code: `def linear(x, weight):
    return [sum(wi*xi for wi,xi in zip(wo,x))
            for wo in weight]
# Wq [16Ã—16] Ã— x [16] = q [16]  (256 Ã§arpma)`,
        highlight: "Transformer'da HER projeksiyon (Wq, Wk, Wv, fc1, fc2) bir matris Ã§arpÄ±mÄ±"
      },
      {
        title: { tr: "Weight Tying & Parametre DaÄŸÄ±lÄ±mÄ±", en: "Weight Tying & Parameter Distribution" },
        viz: "paramDist",
        content: "AynÄ± wte matrisi hem giriÅŸ (lookup) hem Ã§Ä±kÄ±ÅŸ (matris Ã§arpÄ±mÄ±) iÃ§in kullanÄ±lÄ±r â€” weight tying. 3,648 parametrenin %56'sÄ± MLP'de!",
        code: `# GÄ°RÄ°Å: tok_emb = wte[token_id]     # lookup
# Ã‡IKIÅ: logits = linear(x, wte)      # matris Ã§arpÄ±mÄ±
# AynÄ± matris! â†’ 432 param tasarrufu`,
        highlight: "3,648 parametrenin %56'sÄ± MLP'de!"
      },
      {
        title: { tr: "Softmax â€” Skorlardan OlasÄ±lÄ±klara", en: "Softmax â€” From Scores to Probabilities" },
        viz: "softmaxViz",
        content: "Softmax ham skorlarÄ± olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na Ã§evirir: P(i) = exp(xi)/Î£exp(xj). Numerik trick: exp(xi-max) ile overflow Ã¶nlenir.",
        code: `def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    return [e / sum(exps) for e in exps]
# [2.0, 1.0, 0.1] â†’ [0.659, 0.242, 0.099]`,
        highlight: "Numerik trick: exp(xi-max) ile overflow Ã¶nlenir"
      }
    ]
  },

  {
    id: "autograd", week: 2, title: { tr: "Autograd & Backpropagation", en: "Autograd & Backpropagation" }, icon: "â›“ï¸", color: "#F59E0B",
    subtitle: { tr: "Otomatik tÃ¼rev alma, hesaplama grafÄ±, chain rule â€” derin Ã¶ÄŸrenmenin temeli", en: "Automatic differentiation, computation graph, chain rule â€” the foundation of deep learning" },
    sections: [
      {
        title: { tr: "Ã–n Bilgi: TÃ¼rev Nedir? Neden LazÄ±m?", en: "Background: What is a Derivative? Why Do We Need It?" },
        viz: "derivative",
        content: "TÃ¼rev = deÄŸiÅŸim hÄ±zÄ±. f(x) = xÂ² ise f'(3) = 6. Derin Ã¶ÄŸrenmede: âˆ‚L/âˆ‚w = 'w'yi deÄŸiÅŸtirirsem loss ne olur?' Negatif yÃ¶nde gÃ¼ncelle â†’ loss azalÄ±r.",
        highlight: "TÃ¼rev = 'bu parametreyi hangi yÃ¶nde deÄŸiÅŸtirmeliyim ki loss azalsÄ±n?' sorusunun cevabÄ±."
      },
      {
        title: { tr: "Ã–n Bilgi: KÄ±smi TÃ¼rev ve Gradient", en: "Background: Partial Derivatives and Gradients" },
        viz: "derivative",
        content: "f(a,b) = aÃ—b ise âˆ‚f/âˆ‚a = b, âˆ‚f/âˆ‚b = a (diÄŸerini sabit tut). Gradient = tÃ¼m kÄ±smi tÃ¼revlerin vektÃ¶rÃ¼. Gradient descent = gradient'in ters yÃ¶nÃ¼nde adÄ±m at.",
        code: `# f(a,b) = aÃ—b + aÂ²  (a=2, b=3 â†’ f=10)
# âˆ‚f/âˆ‚a = b + 2a = 7  (a'yÄ± 1â†‘ â†’ fâ‰ˆ7â†‘)
# âˆ‚f/âˆ‚b = a = 2        (b'yi 1â†‘ â†’ fâ‰ˆ2â†‘)
# Gradient: âˆ‡f = [7, 2]
# Minimum: a -= lrÃ—7, b -= lrÃ—2`,
        highlight: "Gradient descent = gradient'in ters yÃ¶nÃ¼nde adÄ±m at"
      },
      {
        title: { tr: "ğŸ® Autograd Oyun AlanÄ± â€” CanlÄ± Backward Pass", en: "ğŸ® Autograd Playground â€” Live Backward Pass" },
        viz: "autogradPlayground",
        content: "YukarÄ±da a, b, c deÄŸerlerini kaydÄ±rÄ±cÄ±larla deÄŸiÅŸtirin ve â–¶ Backward butonuna basÄ±n â€” chain rule adÄ±mlarÄ±nÄ±n hesaplama grafÄ± Ã¼zerinde nasÄ±l ilerlediÄŸini canlÄ± izleyin!",
        highlight: "Autograd = derin Ã¶ÄŸrenmeyi mÃ¼mkÃ¼n kÄ±lan teknoloji. Bu kodda ~30 satÄ±rla sÄ±fÄ±rdan yazÄ±lmÄ±ÅŸtÄ±r."
      },
      {
        title: { tr: "Value SÄ±nÄ±fÄ± â€” 4 Temel BileÅŸen", en: "Value Class â€” 4 Core Components" },
        viz: "opGradTable",
        content: "Her sayÄ± Value nesnesi olarak sarmalanÄ±r: data (deÄŸer), grad (tÃ¼rev, baÅŸta 0), _children (girdi dÃ¼ÄŸÃ¼mleri), _local_grads (yerel tÃ¼revler).",
        code: `class Value:
    __slots__ = ('data','grad','_children','_local_grads')
    def __init__(self, data, children=(), local_grads=()):
        self.data = data; self.grad = 0
        self._children = children
        self._local_grads = local_grads`,
        highlight: "Her sayÄ± Value nesnesi olarak sarmalanÄ±r: data (deÄŸer), grad (tÃ¼rev, baÅŸta 0), _"
      },
      {
        title: { tr: "OperatÃ¶r Overloading â€” Otomatik Graf OluÅŸturma", en: "Operator Overloading â€” Automatic Graph Building" },
        viz: "opGradTable",
        content: "a+b, a*b gibi iÅŸlemler otomatik graf oluÅŸturur. Her operasyon yerel gradient'ini bilir: toplamaâ†’(1,1), Ã§arpmaâ†’(b,a), ReLUâ†’(a>0?1:0).",
        code: `def __mul__(self, other):
    other = other if isinstance(other, Value) else Value(other)
    return Value(self.data * other.data,
        (self, other), (other.data, self.data))
# âˆ‚(aÃ—b)/âˆ‚a = b, âˆ‚(aÃ—b)/âˆ‚b = a`,
        highlight: "Her operasyon yerel gradient'ini bilir: toplamaâ†’(1,1), Ã§arpmaâ†’(b,a), ReLUâ†’(a>0?1:0)"
      },
      {
        title: { tr: "Chain Rule & Backward Pass", en: "Chain Rule & Backward Pass" },
        viz: "compGraph",
        content: "Chain rule: f(g(x)) tÃ¼revi = f'(g(x)) Ã— g'(x). TÃ¼revler geri doÄŸru Ã‡ARPILIR. Topological sort ile doÄŸru sÄ±rada, self.grad=1'den baÅŸlayarak hesaplanÄ±r.",
        code: `def backward(self):
    topo = [];
 visited = set()
    def build_topo(v):
        if v not in visited:
            visited.add(v)
            for child in v._children: build_topo(child)
            topo.append(v)
    build_topo(self)
    self.grad = 1  # âˆ‚L/âˆ‚L = 1
    for v in reversed(topo):
        for child, lg in zip(v._children, v._local_grads):
            child.grad += lg * v.grad  # chain rule!`,
        highlight: "Topological sort ile doÄŸru sÄ±rada, self.grad=1'den baÅŸlayarak hesaplanÄ±r"
      },
      {
        title: "Somut Ã–rnek: L = (a Ã— b) + c",
        viz: "compGraph",
        content: "a=2, b=3, c=1 â†’ d=6, L=7. Backward: âˆ‚L/âˆ‚L=1, âˆ‚L/âˆ‚d=1, âˆ‚L/âˆ‚c=1, âˆ‚L/âˆ‚a=b=3, âˆ‚L/âˆ‚b=a=2. Autograd oyun alanÄ±nda bizzat deneyin!",
        highlight: "a.grad=3 â†’ a'yÄ± 1 birim artÄ±rÄ±rsak loss 3 birim artar. Optimizer bu bilgiyi kullanÄ±r."
      },
      {
        title: "Gradient ToplanmasÄ± (+=) Neden Kritik?",
        viz: "compGraph",
        content: "L = aÃ—a ise âˆ‚L/âˆ‚a = 2a. += ile iki yoldan gelen gradient toplanÄ±r â†’ doÄŸru. = ile sadece son yol kalÄ±r â†’ yanlÄ±ÅŸ! Weight tying, residual'da bu durum sÃ¼rekli olur.",
        code: `a = Value(3); L = a * a  # a Ä°KÄ° KEZ kullanÄ±lÄ±r
# += ile: a.grad = 3 + 3 = 6 = 2a  âœ“
# =  ile: a.grad = 3  â† YANLIÅ! (2a=6 olmalÄ±)`,
        highlight: "= ile sadece son yol kalÄ±r â†’ yanlÄ±ÅŸ! Weight tying, residual'da bu durum sÃ¼rekli olur"
      },
      {
        title: "Bu Kod vs. PyTorch KarÅŸÄ±laÅŸtÄ±rma",
        viz: "opGradTable",
        content: "Birebir aynÄ± gradient deÄŸerleri! Fark: Value=skaler (~30 satÄ±r Python), Tensor=N-boyutlu (~100K+ satÄ±r C++/CUDA, GPU ile milyonlarca kat hÄ±zlÄ±).",
        highlight: "Her eÄŸitim adÄ±mÄ±nda p.grad = 0 yapÄ±lmazsa gradient birikir â†’ model patlar!"
      }
    ]
  },

  {
    id: "attention", week: 3, title: { tr: "Self-Attention MekanizmasÄ±", en: "Self-Attention Mechanism" }, icon: "ğŸ”", color: "#10B981",
    subtitle: { tr: "QÂ·Káµ€/âˆšd â†’ Softmax â†’ V â€” Transformer'Ä±n kalbi", en: "QÂ·Káµ€/âˆšd â†’ Softmax â†’ V â€” The heart of the Transformer" },
    sections: [
      {
        title: "Ã–n Bilgi: RNN'den Attention'a",
        viz: "rnnToAttn",
        content: "2017 Ã¶ncesi RNN/LSTM standardÄ±: sÄ±ralÄ±, yavaÅŸ, uzun mesafe baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± unutur. 'Attention Is All You Need' â†’ RNN'yi kaldÄ±r, SADECE attention kullan = Transformer.",
        highlight: "Transformer = Attention + Feed-Forward, RNN yok. Bu basit fikir tÃ¼m modern AI'Ä± mÃ¼mkÃ¼n kÄ±ldÄ±."
      },
      {
        title: "Self-Attention â€” Sezgisel Anlama",
        viz: "attentionFlow",
        content: "Her token Ã¶nceki tokenlara bakarak bilgi toplar. BazÄ±larÄ±na Ã§ok dikkat eder (yÃ¼ksek aÄŸÄ±rlÄ±k), bazÄ±larÄ±nÄ± yok sayar. Bu aÄŸÄ±rlÄ±klar dinamik â€” her girdi iÃ§in yeniden hesaplanÄ±r.",
        highlight: "Attention = dinamik, veri-baÄŸÄ±mlÄ± aÄŸÄ±rlÄ±klama. Statik deÄŸil â€” her girdi iÃ§in farklÄ±."
      },
      {
        title: "Query, Key, Value â€” 3 FarklÄ± Rol",
        viz: "attentionFlow",
        content: "Q = 'ne arÄ±yorum?', K = 'bende ne var?', V = 'bilgim nedir'. QÂ·K dot product = uyum skoru. YÃ¼ksek skor â†’ V'den daha Ã§ok bilgi al.",
        code: `q = linear(x, state_dict['attn_wq'])  # [16]â†’[16]
k = linear(x, state_dict['attn_wk'])
v = linear(x, state_dict['attn_wv'])
# QÂ·K yÃ¼ksek â†’ o token'a Ã§ok dikkat et`,
        highlight: "YÃ¼ksek skor â†’ V'den daha Ã§ok bilgi al"
      },
      {
        title: "ğŸ® Attention Oyun AlanÄ± â€” Head KalÄ±plarÄ±nÄ± KeÅŸfet",
        viz: "attentionPlayground",
        content: "YukarÄ±da 4 farklÄ± head seÃ§ip 'Banana' kelimesinde her token'Ä±n nelere dikkat ettiÄŸini gÃ¶rÃ¼n. SatÄ±rlara tÄ±klayarak dikkat daÄŸÄ±lÄ±mÄ±nÄ± inceleyin â€” her head farklÄ± kalÄ±p Ã¶ÄŸrenir!",
        highlight: "Her head baÄŸÄ±msÄ±z attention hesabÄ± yapar. Biri sesli-sessiz uyumunu, diÄŸeri pozisyon yakÄ±nlÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenebilir."
      },
      {
        title: "Scaled Dot-Product â€” Tam Hesaplama",
        viz: "attentionFlow",
        content: "4 adÄ±m: QÂ·K (skor) â†’ Ã·âˆšd (scaling) â†’ softmax (olasÄ±lÄ±k) â†’ Ã—V (bilgi toplama). âˆšd bÃ¶lme kritik: boyut bÃ¼yÃ¼yÃ¼nce softmax spike yapar, gradient kaybolur.",
        code: `for t in range(len(keys)):
    score[t] = dot(q, keys[t]) / sqrt(head_dim)
weights = softmax(scores)
out = weighted_sum(weights, values)
# Attention(Q,K,V) = softmax(QÂ·Káµ€/âˆšd)Â·V`,
        highlight: "âˆšd bÃ¶lme kritik: boyut bÃ¼yÃ¼yÃ¼nce softmax spike yapar, gradient kaybolur"
      },
      {
        title: "Ã–n Bilgi: Dot Product â€” Benzerlik Ã–lÃ§Ã¼mÃ¼",
        viz: "dotProduct",
        content: "aÂ·b = Î£ aáµ¢Ã—báµ¢. AynÄ± yÃ¶n â†’ pozitif (benzer), dik â†’ 0 (ilgisiz), ters â†’ negatif (zÄ±t). QÂ·K = sorgu ile anahtar ne kadar uyumlu?",
        code: `# qÂ·k1 = 0.2+0.06+0.02+0.03 = 0.31 (benzer!)
# qÂ·k2 = -0.25-0.09-0.04-0.01 = -0.39 (zÄ±t)
# â†’ q, k1'e daha Ã§ok dikkat edecek`,
        highlight: "QÂ·K = sorgu ile anahtar ne kadar uyumlu?"
      },
      {
        title: "Multi-Head & Causal Masking",
        viz: "causalMask",
        content: "16 boyut â†’ 4 head Ã— 4 dim. Her head baÄŸÄ±msÄ±z attention yapar, sonra birleÅŸtirilip Wo ile projekte edilir. Causal mask: her token sadece Ã–NCEKÄ°LERÄ° gÃ¶rÃ¼r.",
        code: `# Multi-head: her head 4 boyutluk dilim
for h in range(4):
    q_h = q[h*4:(h+1)*4]
    # baÄŸÄ±msÄ±z attention â†’ concatenate â†’ Wo
# Causal: KV cache = doÄŸal mask`,
        highlight: "Causal mask: her token sadece Ã–NCEKÄ°LERÄ° gÃ¶rÃ¼r"
      },
      {
        title: "Attention Ã‡Ä±ktÄ±sÄ± & Linear Projeksiyon",
        viz: "residualViz",
        content: "4 head birleÅŸtirilir (4Ã—4=16), Wo matrisi ile projekte edilir, residual eklenir: x = WoÃ—heads + x_residual. Wo sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity.",
        code: `# 6 linear projeksiyon: Wq, Wk, Wv, Wo, fc1, fc2
# Hepsi aynÄ±: x Ã— W (bias yok, modern trend)`,
        highlight: "Wo sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity"
      }
    ]
  },

  {
    id: "transformer", week: 4, title: { tr: "Transformer BloklarÄ±", en: "Transformer Blocks" }, icon: "ğŸ§±", color: "#EC4899",
    subtitle: { tr: "RMSNorm, MLP, Residual â€” tam mimari, katman katman", en: "RMSNorm, MLP, Residual â€” full architecture, layer by layer" },
    sections: [
      {
        title: "Genel Mimari â€” BÃ¼yÃ¼k Resim",
        viz: "archPipeline",
        content: "Bir Transformer katmanÄ± = Attention + MLP. Her blok Ã¶ncesi RMSNorm, sonrasÄ± residual. AkÄ±ÅŸ: x â†’ norm â†’ attn â†’ +x â†’ norm â†’ MLP â†’ +x â†’ Ã§Ä±ktÄ±.",
        highlight: "Pre-norm: norm â†’ block â†’ +residual. Modern modellerin standardÄ± â€” daha kararlÄ± eÄŸitim."
      },
      {
        title: "ğŸ® Transformer BloÄŸu â€” AdÄ±m AdÄ±m SimÃ¼lasyon",
        viz: "transformerBlockFlow",
        content: "â–¶ AkÄ±ÅŸ butonuna basÄ±n ve 8 aÅŸamayÄ± sÄ±rayla izleyin: girdi â†’ RMSNorm â†’ Attention â†’ +Residual â†’ RMSNorm â†’ MLP â†’ +Residual â†’ Ã§Ä±ktÄ±. Kutulara tÄ±klayarak adÄ±m atlayÄ±n.",
        highlight: "Her aÅŸama bir iÅŸlev: Norm=kararlÄ±lÄ±k, Attn=bilgi toplama, MLP=bilgi iÅŸleme, Residual=gradient highway."
      },
      {
        title: "RMSNorm â€” NasÄ±l Ã‡alÄ±ÅŸÄ±r?",
        viz: "normCompare",
        content: "RMS = âˆšmean(xÂ²). Her elemanÄ± RMS'e bÃ¶l â†’ normalize. LayerNorm'dan farkÄ±: ortalama Ã§Ä±karmaz â†’ %30 daha hÄ±zlÄ±, eÅŸdeÄŸer kalite.",
        code: `def rmsnorm(x):
    ms = sum(xi*xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]`,
        highlight: "LayerNorm'dan farkÄ±: ortalama Ã§Ä±karmaz â†’ %30 daha hÄ±zlÄ±, eÅŸdeÄŸer kalite"
      },
      {
        title: "MLP Bloku â€” Feed-Forward Network",
        viz: "mlpFlow",
        content: "Her token'Ä± baÄŸÄ±msÄ±z iÅŸler: geniÅŸlet (16â†’64) â†’ ReLUÂ² aktive â†’ daralt (64â†’16). ~%40 nÃ¶ron 'Ã¶lÃ¼' kalÄ±r (sparse = iyi!).",
        code: `h = linear(x, state_dict['mlp_fc1'])  # 16â†’64
h = [xi.relu()**2 for xi in h]        # ReLUÂ²
x = linear(h, state_dict['mlp_fc2'])  # 64â†’16
x = [a+b for a,b in zip(x, x_res)]   # +residual`,
        highlight: "~%40 nÃ¶ron 'Ã¶lÃ¼' kalÄ±r (sparse = iyi!)"
      },
      {
        title: "Ã–n Bilgi: Aktivasyon Fonksiyonu Neden Gerekli?",
        viz: "activation",
        content: "Aktivasyon olmadan derin aÄŸ = tek matris Ã§arpÄ±mÄ± (Wâ‚ƒÃ—Wâ‚‚Ã—Wâ‚Ã—x = WÃ—x). Non-linearity her katmana farklÄ± karar sÄ±nÄ±rÄ± Ã¶ÄŸretir.",
        highlight: "Aktivasyon = non-linearity. Onsuz derin aÄŸ = basit matris Ã§arpÄ±mÄ±. TÃ¼m gÃ¼Ã§ buradan gelir."
      },
      {
        title: "Residual BaÄŸlantÄ±lar â€” Gradient Highway",
        viz: "residualViz",
        content: "x = f(x) + x. Gradient doÄŸrudan giriÅŸe akar: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out Ã— (âˆ‚f/âˆ‚x + 1). +1 terimi = kestirme yol. Wo ve fc2 sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity.",
        code: `# Attention: x = attention(norm(x)) + x_res
# MLP:       x = mlp(norm(x)) + x_res
# Gradient: +1 terimi gradient'in kaybolmasÄ±nÄ± Ã¶nler`,
        highlight: "Wo ve fc2 sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity"
      },
      {
        title: "Weight Initialization â€” Kritik BaÅŸlatma",
        viz: "weightInit",
        content: "Genel parametreler: std=0.08 â‰ˆ 1/âˆšn_embd. Wo ve fc2: std=0 â†’ baÅŸta residual = identity. Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ kaybolma.",
        code: `# Genel: random.gauss(0, 0.08)
# Wo, fc2: random.gauss(0, 0.0)  â† sÄ±fÄ±ra yakÄ±n
# â†’ BaÅŸta: x â‰ˆ x + 0 = x (identity)`,
        highlight: "Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ kaybolma"
      },
      {
        title: "RMSNorm vs LayerNorm â€” KarÅŸÄ±laÅŸtÄ±rma",
        viz: "normCompare",
        content: "LayerNorm: Î¼ Ã§Ä±kar, Ïƒ'ya bÃ¶l, Î³ ve Î² uygula (4 iÅŸlem). RMSNorm: sadece RMS'e bÃ¶l, Î³ uygula (2 iÅŸlem). LLaMA, Mistral, Gemma hep RMSNorm.",
        highlight: "Ortalama Ã§Ä±karmak gereksiz bulundu â€” %30 hÄ±z kazancÄ±, sÄ±fÄ±r kalite kaybÄ±."
      }
    ]
  },

  {
    id: "training", week: 5, title: { tr: "EÄŸitim DÃ¶ngÃ¼sÃ¼", en: "Training Loop" }, icon: "ğŸ”„", color: "#EF4444",
    subtitle: { tr: "Loss, optimizer, learning rate â€” modeli Ã¶ÄŸretme sanatÄ±", en: "Loss, optimizer, learning rate â€” the art of teaching a model" },
    sections: [
      {
        title: "Ã–n Bilgi: Optimizasyon Nedir?",
        viz: "gradDescent",
        content: "Optimizasyon = loss fonksiyonunun minimumunu bulma. 3,648 boyutlu uzayda en alÃ§ak noktayÄ± arÄ±yoruz. Gradient descent = her adÄ±mda gradient'in ters yÃ¶nÃ¼nde kÃ¼Ã§Ã¼k adÄ±m.",
        highlight: "Derin Ã¶ÄŸrenme = fonksiyon optimizasyonu. Loss'u minimize eden parametreleri bulmak = tÃ¼m iÅŸ."
      },
      {
        title: "Gradient Descent â€” Sezgisel Anlama",
        viz: "gradDescent",
        content: "GÃ¶zleri kapalÄ± daÄŸda: en dik yokuÅŸu hissedip (gradient) tersine adÄ±m at (gÃ¼ncelleme). AdÄ±m boyutu = learning rate. Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ yavaÅŸ.",
        code: `p.data -= learning_rate * p.grad
# grad>0 â†’ p azalt, grad<0 â†’ p artÄ±r, grad=0 â†’ minimum`,
        highlight: "Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ yavaÅŸ"
      },
      {
        title: "ğŸ® EÄŸitim SimÃ¼lasyonu â€” LR Etkisini Dene",
        viz: "trainingSim",
        content: "YukarÄ±da learning rate kaydÄ±rÄ±cÄ±sÄ±nÄ± ayarlayÄ±p â–¶ EÄŸit butonuna basÄ±n. Loss eÄŸrisinin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶zlemleyin: Ã§ok yÃ¼ksek LR â†’ patlama, Ã§ok dÃ¼ÅŸÃ¼k â†’ yavaÅŸ Ã¶ÄŸrenme, doÄŸru LR â†’ gÃ¼zel dÃ¼ÅŸÃ¼ÅŸ!",
        highlight: "LR Ã§ok bÃ¼yÃ¼k â†’ diverge, Ã§ok kÃ¼Ã§Ã¼k â†’ Ã§ok yavaÅŸ. Ä°yi bÃ¶lge: 0.005-0.05 arasÄ±."
      },
      {
        title: "Cross-Entropy Loss",
        viz: "lossTable",
        content: "L = -log(P(doÄŸru_token)). P=1â†’L=0, P=1/27â†’Lâ‰ˆ3.33 (rastgele). EÄŸitimle loss dÃ¼ÅŸer: 3.33 â†’ 2.8 â†’ 2.0 â†’ 1.5.",
        code: `loss_t = -probs[target_id].log()
loss = (1/n) * sum(losses)  # ortalama
# BaÅŸlangÄ±Ã§ â‰ˆ 3.33, hedef < 2.0`,
        highlight: "EÄŸitimle loss dÃ¼ÅŸer: 3.33 â†’ 2.8 â†’ 2.0 â†’ 1.5"
      },
      {
        title: "Ã–n Bilgi: Log Fonksiyonu â€” Neden?",
        viz: "crossEntropyGraph",
        content: "-log(p): p=1â†’0, pâ†’0â†’âˆ. DÃ¼ÅŸÃ¼k olasÄ±lÄ±ÄŸa aÄŸÄ±r ceza. Ã‡arpÄ±mlarÄ± toplama Ã§evirir (numerik kararlÄ±lÄ±k). Bilgi teorisi: sÃ¼rpriz Ã¶lÃ§Ã¼sÃ¼.",
        highlight: "-log(p) = sÃ¼rpriz. Beklenmeyen olay â†’ yÃ¼ksek sÃ¼rpriz â†’ yÃ¼ksek loss. Model sÃ¼rprizi minimize eder."
      },
      {
        title: "Adam Optimizer â€” SGD'nin Evrimi",
        viz: "adamEvolution",
        content: "SGD sorunlarÄ±: tek lr, gÃ¼rÃ¼ltÃ¼ye duyarlÄ±. Adam = Momentum (yÃ¶n) + RMSprop (Ã¶lÃ§ek). Her parametre kendi adaptif lr'Ä±nÄ± alÄ±r. NLP standardÄ±.",
        code: `m[i] = 0.85*m[i] + 0.15*p.grad      # momentum
v[i] = 0.99*v[i] + 0.01*p.grad**2   # adaptif Ã¶lÃ§ek
p.data -= lr * m_hat / (v_hat**0.5 + 1e-8)
p.grad = 0  # â† KRÄ°TÄ°K sÄ±fÄ±rlama!`,
        highlight: "SGD sorunlarÄ±: tek lr, gÃ¼rÃ¼ltÃ¼ye duyarlÄ±"
      },
      {
        title: "Learning Rate Decay & EÄŸitim DÃ¶ngÃ¼sÃ¼",
        viz: "lrDecay",
        content: "Linear decay: lr_t = lr Ã— (1 - step/num_steps). Modern: warmup + cosine decay. Tam dÃ¶ngÃ¼: forward â†’ loss â†’ backward â†’ adam gÃ¼ncelle â†’ grad sÄ±fÄ±rla â†’ tekrarla.",
        code: `lr_t = learning_rate * (1 - step / num_steps)
# step=0: lr=0.01, step=500: 0.005, step=1000: 0.0`,
        highlight: "Tam dÃ¶ngÃ¼: forward â†’ loss â†’ backward â†’ adam gÃ¼ncelle â†’ grad sÄ±fÄ±rla â†’ tekrarla"
      },
      {
        title: "Gradient SÄ±fÄ±rlama â€” Neden Åart?",
        viz: "trainingCycle",
        content: "Backward += ile gradient biriktirir. SÄ±fÄ±rlanmazsa: 0.5 â†’ 0.8 â†’ 1.5 â†’ âˆ â†’ model patlar! Her adÄ±mda p.grad = 0 yapÄ±lmalÄ±.",
        code: `for i, p in enumerate(params):
    p.data -= lr_t * m_hat / (v_hat**0.5 + 1e-8)
    p.grad = 0  # â† BU SATIR OLMADAN MODEL PATLAR`,
        highlight: "SÄ±fÄ±rlanmazsa: 0.5 â†’ 0.8 â†’ 1.5 â†’ âˆ â†’ model patlar! Her adÄ±mda p.grad = 0 yapÄ±lmalÄ±"
      }
    ]
  },

  {
    id: "inference", week: 6, title: "Inference & Text Generation", icon: "âœ¨", color: "#6366F1",
    subtitle: { tr: "Autoregressive sampling, temperature, KV cache â€” modelin konuÅŸma zamanÄ±", en: "Autoregressive sampling, temperature, KV cache â€” when the model speaks" },
    sections: [
      {
        title: "EÄŸitim vs Inference â€” Fark Nedir?",
        viz: "inferenceTimeline",
        content: "EÄŸitim: forward + backward + gÃ¼ncelle (paralel, gradient gerekli). Inference: sadece forward (sÄ±ralÄ±, gradient yok â†’ hÄ±zlÄ±, az bellek). EÄŸitim = sÄ±nav Ã§alÄ±ÅŸmak, Inference = sÄ±nav vermek.",
        highlight: "Inference'da backward pass YOK â†’ daha az bellek, daha hÄ±zlÄ±. Ama autoregressive olduÄŸu iÃ§in sÄ±ralÄ±."
      },
      {
        title: "Autoregressive Generation â€” AdÄ±m AdÄ±m",
        viz: "inferenceTimeline",
        content: "BOS ile baÅŸla â†’ model Ã§alÄ±ÅŸtÄ±r â†’ 27 olasÄ±lÄ±k â†’ temperature Ã¶lÃ§ekle â†’ softmax â†’ Ã¶rnekle â†’ BOS/EOS ise DUR, deÄŸilse tekrarla. SonuÃ§: 'kamrin' gibi yeni isimler!",
        code: `token_id = BOS; sample = []
for pos_id in range(block_size):
    logits = gpt(token_id, pos_id, keys, values)
    probs = softmax([l/temperature for l in logits])
    token_id = random.choices(range(27), weights=probs)[0]
    if token_id == BOS: break
    sample.append(uchars[token_id])`,
        highlight: "SonuÃ§: 'kamrin' gibi yeni isimler!"
      },
      {
        title: "ğŸ® Ãœretim Oyun AlanÄ± â€” Temperature Etkisini Dene",
        viz: "generationPlayground",
        content: "YukarÄ±da temperature kaydÄ±rÄ±cÄ±sÄ±nÄ± ayarlayÄ±p â–¶ Ãœret butonuna basÄ±n. DÃ¼ÅŸÃ¼k T â†’ her zaman aynÄ± isim, yÃ¼ksek T â†’ kaotik harfler. Softmax daÄŸÄ±lÄ±m ÅŸeklinin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶zlemleyin!",
        highlight: "DÃ¼ÅŸÃ¼k T â†’ deterministik (tekrar), Tâ‰ˆ0.8 â†’ dengeli (yaratÄ±cÄ± ama gerÃ§ekÃ§i), yÃ¼ksek T â†’ kaotik."
      },
      {
        title: "Temperature Scaling â€” YaratÄ±cÄ±lÄ±k KontrolÃ¼",
        viz: "temperatureViz",
        content: "probs = softmax(logits/T). Tâ†’0: greedy (deterministik). T=0.8: dengeli. T=1: orijinal daÄŸÄ±lÄ±m. T>1: dÃ¼z (rastgele). Fizik analojisi: dÃ¼ÅŸÃ¼k T â†’ dÃ¼zenli, yÃ¼ksek T â†’ kaotik.",
        code: `# T=0.5: [0.876, 0.117, 0.007] â† sivri
# T=1.0: [0.659, 0.242, 0.099] â† dengeli
# T=2.0: [0.387, 0.337, 0.276] â† dÃ¼z`,
        highlight: "Fizik analojisi: dÃ¼ÅŸÃ¼k T â†’ dÃ¼zenli, yÃ¼ksek T â†’ kaotik"
      },
      {
        title: "Sampling Stratejileri",
        viz: "samplingViz2",
        content: "Bu kodda: random sampling (tam daÄŸÄ±lÄ±mdan). Alternatifler: Greedy (argmax), Top-k (en yÃ¼ksek k token), Top-p/Nucleus (kÃ¼mÃ¼latif olasÄ±lÄ±k eÅŸiÄŸi). Production: genelde top-p + temperature.",
        highlight: "Bu kod en basit stratejiyi kullanÄ±r. Production modeller genelde top-p (nucleus) tercih eder."
      },
      {
        title: "KV Cache â€” O(nÂ²) â†’ O(n)",
        viz: "kvCache",
        content: "Ã–nceki pozisyonlarÄ±n K,V vektÃ¶rlerini sakla. Her yeni token iÃ§in sadece 1 K,V hesapla, Ã¶ncekiler cache'ten oku. Bu kodda Python listesi, production'da Paged Attention.",
        code: `keys[layer_idx].append(k)    # cache'e ekle
values[layer_idx].append(v)
# pos=5: keys = [kâ‚€,...,kâ‚…] â† Ã¶ncekiler hazÄ±r!`,
        highlight: "Bu kodda Python listesi, production'da Paged Attention"
      },
      {
        title: "Inference Pipeline â€” UÃ§tan Uca Ã–rnek",
        viz: "inferenceTimeline",
        content: "'kamrin': BOSâ†’'k'(P=.08)â†’'a'(.15)â†’'m'(.11)â†’'r'(.09)â†’'i'(.22)â†’'n'(.18)â†’BOS(.31)â†’DUR. Veri setinde yok ama Ä°ngilizce yapÄ±sÄ±na uygun!",
        highlight: "Model olasÄ±lÄ±k Ã¼retir, sampling seÃ§er. FarklÄ± temperature/seed â†’ farklÄ± isimler."
      },
      {
        title: "Bu Kod vs Production GPT â€” KapanÄ±ÅŸ",
        viz: "whatsMissing",
        content: "Birebir aynÄ± algoritma! Fark sadece Ã¶lÃ§ek: 3,648 vs 1T+ parametre, 8 vs 128K+ context, dakikalar vs aylar eÄŸitim, $0 vs $100M+ maliyet. TEMELDEKÄ° MATEMATÄ°K AYNI. Tebrikler!",
        highlight: "243 satÄ±r Python ile GPT'nin tÃ¼m temellerini Ã¶ÄŸrendiniz. ArtÄ±k 'sihir' deÄŸil, anlaÅŸÄ±lÄ±r matematik!"
      }
    ]
  },

  {
    id: "evolution", week: 7, title: { tr: "Modern AI'a Evrim", en: "Evolution to Modern AI" }, icon: "ğŸŒ", color: "#14B8A6",
    subtitle: { tr: "243 satÄ±rdan ChatGPT'ye â€” Ã¶lÃ§ek, donanÄ±m, yazÄ±lÄ±m ve paradigma deÄŸiÅŸimleri", en: "From 243 lines to ChatGPT â€” scale, hardware, software, and paradigm shifts" },
    sections: [
      {
        title: "Scaling Laws â€” Daha BÃ¼yÃ¼k = Daha Ä°yi?",
        viz: "scalingLaws",
        content: "Kaplanick et al. (2020): loss âˆ 1/N^Î±. Parametre, veri ve hesaplama artÄ±nca loss dÃ¼ÅŸer â€” gÃ¼Ã§ yasasÄ± iliÅŸkisi. Chinchilla (2022): optimal oran = 20 token/parametre.",
        code: `# Scaling Law (Kaplanick et al. 2020):
# L(N) = a / N^b  (N=parametre sayÄ±sÄ±)
# microGPT:  N=3,648   â†’ loss â‰ˆ 2.0
# GPT-2:     N=1.5B    â†’ loss â‰ˆ 0.8
# GPT-3:     N=175B    â†’ loss â‰ˆ 0.5
# Chinchilla: optimal D/N â‰ˆ 20 (token/param)`,
        highlight: "microGPT'den GPT-4'e geÃ§iÅŸ 'yeni matematik' deÄŸil, AYNI matematiÄŸin 1 milyon kat bÃ¼yÃ¼tÃ¼lmesi."
      },
      {
        title: "ğŸ® Evrim Zaman Ã‡izelgesi â€” GPT-1'den BugÃ¼ne",
        viz: "evolutionTimeline",
        content: "AÅŸaÄŸÄ±daki zaman Ã§izelgesinde her modele tÄ±klayÄ±p parametre, veri, yenilik ve maliyet bilgilerini inceleyin. microGPT'den GPT-4'e 6 bÃ¼yÃ¼klÃ¼k mertebesi fark â€” ama temel aynÄ±!",
        highlight: "2018'den 2024'e: 117M â†’ 1.8T parametre, $10K â†’ $100M+ maliyet. Ama Transformer temeli hiÃ§ deÄŸiÅŸmedi."
      },
      {
        title: "DonanÄ±m Evrimi â€” CPU â†’ GPU â†’ TPU",
        viz: "hardwareEvolution",
        content: "CPU: sÄ±ralÄ±, genel amaÃ§lÄ±. GPU: binlerce kÃ¼Ã§Ã¼k Ã§ekirdek, paralel matris Ã§arpÄ±mÄ± (NVIDIA A100: 312 TFLOPS). TPU: Google'Ä±n Ã¶zel AI Ã§ipi. Yeni: Groq LPU, Cerebras WSE.",
        highlight: "GPU olmadan modern AI yok. A100 tek Ã§ipte 80GB bellek, 312 TFLOPS â€” microGPT'yi saniyede milyonlarca kez Ã§alÄ±ÅŸtÄ±rÄ±r."
      },
      {
        title: "EÄŸitim Pipeline'Ä± â€” Pre-training â†’ RLHF",
        viz: "trainingPipeline",
        content: "3 aÅŸama: (1) Pre-training: internet-Ã¶lÃ§eÄŸinde metin, next-token prediction â€” bu derste Ã¶ÄŸrendiÄŸiniz! (2) SFT: insan yazÄ±mÄ± soru-cevap ile fine-tune. (3) RLHF/DPO: insan tercihleri ile hizalama.",
        code: `# 3-AÅŸamalÄ± Modern LLM EÄŸitimi:
# 1. Pre-training (bu ders!):
#    loss = -log(P(next_token | prev_tokens))
#    Veri: internet (~13T token)
#    Maliyet: ~%95 toplam bÃ¼tÃ§e

# 2. SFT (Supervised Fine-Tuning):
#    (soru, cevap) Ã§iftleri ile fine-tune
#    ~100K Ã¶rnek

# 3. RLHF (Human Alignment):
#    reward_model = train(human_preferences)
#    policy = PPO(model, reward_model)`,
        highlight: "Pre-training = ham gÃ¼Ã§, SFT = yetenek, RLHF = 'iyi davranÄ±ÅŸ'. microGPT sadece adÄ±m 1'i yapÄ±yor."
      },
      {
        title: "Tokenization Evrimi â€” Karakter â†’ BPE â†’ SentencePiece",
        viz: "tokenEvolution",
        content: "microGPT: karakter dÃ¼zeyi (27 token). BPE (GPT-2): alt-kelime (~50K token, 'playing'â†’'play'+'ing'). SentencePiece (LLaMA): Unicode-aware. tiktoken (GPT-4): ~100K token.",
        highlight: "Daha iyi tokenizer = daha az token = daha uzun context = daha iyi anlama. Ama temel fikir aynÄ±: metinâ†’ID."
      },
      {
        title: "Attention Evrimi â€” Vanilla â†’ Flash â†’ Sliding Window",
        viz: "attentionEvolution",
        content: "Vanilla: O(nÂ²) bellek ve hesaplama. Multi-Query (2019): K,V paylaÅŸÄ±mÄ±. Flash Attention (2022): IO-aware â†’ 2-4Ã— hÄ±zlÄ±. Sliding Window (Mistral): sabit pencere â†’ âˆ context.",
        highlight: "Flash Attention = aynÄ± matematik, farklÄ± bellek eriÅŸim dÃ¼zeni. SonuÃ§ deÄŸiÅŸmez, hÄ±z 4Ã— artar."
      },
      {
        title: "AÃ§Ä±k Kaynak Devrimi â€” LLaMA, Mistral, DeepSeek",
        viz: "opensourceMap",
        content: "2023 kÄ±rÄ±lma noktasÄ±: LLaMA â†’ aÃ§Ä±k aÄŸÄ±rlÄ±klar, araÅŸtÄ±rma patlamasÄ±. Mistral 7B: kÃ¼Ã§Ã¼k ama gÃ¼Ã§lÃ¼. DeepSeek-V3: MoE, Ã¼cretsiz API. Qwen, Gemma, Command-R: Ã§eÅŸitlilik.",
        highlight: "AÃ§Ä±k kaynak modeller GPT-4 seviyesine yaklaÅŸÄ±yor. microGPT'nin prensipleri bunlarÄ±n HEPSÄ°NDE aynÄ±."
      },
      {
        title: "GÃ¼ncel Trendler â€” MoE, RAG, Agent, Multimodal",
        viz: "trendsRadar",
        content: "MoE: 8 uzman, her token 2 uzmanÄ± aktive eder â†’ verimlilik. RAG: dÄ±ÅŸ bilgi ile zenginleÅŸtirme. Agent: araÃ§ kullanÄ±mÄ± (kod, arama). Multimodal: metin+gÃ¶rÃ¼ntÃ¼+ses.",
        highlight: "Her trend, bu derste Ã¶ÄŸrendiÄŸiniz Transformer temelinin Ã¼stÃ¼ne inÅŸa edilir. Temel saÄŸlam = her ÅŸey mÃ¼mkÃ¼n."
      }
    ]
  },


  {
    id: "advanced", week: 8, title: { tr: "Ä°leri DÃ¼zey Teknikler & AraÅŸtÄ±rma YÃ¶ntemleri", en: "Advanced Techniques & Research Methods" }, icon: "ğŸ”¬", color: "#E11D48",
    subtitle: { tr: "BPE bilgi teorisi, Hessian, pruning, isotropy, numerik stabilite, akademik yazÄ±m", en: "BPE information theory, Hessian, pruning, isotropy, numerical stability, academic writing" },
    sections: [
      {
        title: { tr: "BPE'nin Bilgi-Teorik Temeli", en: "Information-Theoretic Foundation of BPE" },
        viz: "bpeInfoTheory",
        content: "BPE neden 'en sÄ±k komÅŸu Ã§ifti' birleÅŸtirir? Ã‡Ã¼nkÃ¼ sÄ±k Ã§iftler â†’ dÃ¼ÅŸÃ¼k entropi â†’ minimum description length (MDL). Bu, Huffman kodlama ile aynÄ± sezgidir: sÄ±k olanÄ± kÄ±sa tut.",
        highlight: "BPE = greedy MDL sÄ±kÄ±ÅŸtÄ±rma. Her birleÅŸtirme toplam entropi'yi dÃ¼ÅŸÃ¼rÃ¼r: H(corpus) â†“ = daha verimli kodlama.",
        code: "# BPE Merge = Entropi DÃ¼ÅŸÃ¼ÅŸÃ¼:\n# merge('e','s') â†’ 'es' (freq=1847)\n# H_before = -Î£ p_i log p_i = 4.23 bits\n# H_after  = -Î£ p_i log p_i = 4.18 bits\n# Î”H = -0.05 bits â†’ daha verimli!\n\n# Optimal vocab bÃ¼yÃ¼klÃ¼ÄŸÃ¼:\n# Too small: Hâ†‘ (uzun diziler)\n# Too large: Hâ†‘ (sparse embedding)\n# Sweet spot: 32K-100K token"
      },
      {
        title: { tr: "Hessian Matrisi â€” Ä°kinci TÃ¼rev Bilgisi", en: "Hessian Matrix â€” Second-Order Information" },
        viz: "hessianLandscape",
        content: "Gradient (1. tÃ¼rev) = 'hangi yÃ¶ne git'. Hessian (2. tÃ¼rev) = 'yÃ¼zey ne kadar eÄŸri/dÃ¼z'. DÃ¼z minimum â†’ daha iyi genelleme. Keskin minimum â†’ overfit riski.",
        highlight: "Newton metodu: w â† w - Hâ»Â¹g. Hessian'Ä± hesaplamak O(nÂ²) â€” GPT'de imkansÄ±z! YaklaÅŸÄ±mlar: Fisher, K-FAC, Gauss-Newton.",
        code: "# Hessian hesaplama (kÃ¼Ã§Ã¼k model):\nimport torch.autograd.functional as F\nH = F.hessian(loss_fn, params)\n\n# Eigenvalue analizi:\nevals = torch.linalg.eigvalsh(H)\nsharpness = evals.max()  # keskinlik\nflatness = 1.0 / sharpness\n\n# microGPT: 3648 parametre\n# H boyutu: 3648 Ã— 3648 = 13.3M eleman"
      },
      {
        title: { tr: "ğŸ”¬ Attention Head Pruning â€” Taylor Expansion", en: "ğŸ”¬ Attention Head Pruning â€” Taylor Expansion" },
        viz: "headPruning",
        content: "TÃ¼m attention headlar eÅŸit deÄŸil! Taylor expansion ile her head'in loss'a katkÄ±sÄ±nÄ± Ã¶lÃ§Ã¼p gereksiz olanlarÄ± Ã§Ä±karabiliriz. |âˆ‚L/âˆ‚h Ã— h| = head'in Ã¶nemi.",
        highlight: "GPT-2: 12 head Ã— 12 katman = 144 head. Tipik olarak %30-40'Ä± prune edilebilir â€” hÄ±z artÄ±ÅŸÄ±, minimal kayÄ±p.",
        code: "# Head importance skoru (Taylor 1st order):\ndef head_importance(model, data):\n    scores = []\n    for layer in model.transformer.h:\n        attn_out = layer.attn(x)  # [B,T,D]\n        grad = torch.autograd.grad(loss, attn_out)\n        importance = (grad * attn_out).abs().sum()\n        scores.append(importance)\n    return scores\n\n# Prune en dÃ¼ÅŸÃ¼k %30:\nthreshold = sorted(scores)[int(0.3 * len(scores))]\nmask = [s > threshold for s in scores]"
      },
      {
        title: { tr: "Embedding Ä°zotropi â€” Neden Ã–nemli?", en: "Embedding Isotropy â€” Why Does It Matter?" },
        viz: "isotropyViz",
        content: "Ä°zotrop = vektÃ¶rler uzayda eÅŸit daÄŸÄ±lmÄ±ÅŸ. Anizotrop = dar bir konide toplanmÄ±ÅŸ. Ã‡oÄŸu LLM embedding'i anizotrop â†’ benzerlik Ã¶lÃ§Ã¼mÃ¼ bozulur.",
        highlight: "Cosine similarity hepsi ~0.95 ise 'her ÅŸey birbirine benzer' = iÅŸe yaramaz. DÃ¼zeltme: whitening, normalization.",
        code: "# Ä°zotropi Ã¶lÃ§Ã¼mÃ¼:\ndef isotropy(embeddings):\n    # TÃ¼m Ã§ift cosine similarity\n    norms = embeddings / embeddings.norm(dim=-1, keepdim=True)\n    sim_matrix = norms @ norms.T\n    # Ortalama off-diagonal similarity\n    mask = ~torch.eye(len(embeddings), dtype=bool)\n    avg_sim = sim_matrix[mask].mean()\n    # Ä°zotrop â†’ avg_sim â‰ˆ 0, Anizotrop â†’ avg_sim â†’ 1\n    return 1.0 - avg_sim.item()  # 1=tam izotrop"
      },
      {
        title: { tr: "Float16 Numerik Stabilite â€” Softmax Overflow", en: "Float16 Numerical Stability â€” Softmax Overflow" },
        viz: "numericalStability",
        content: "Float16 max: 65,504. Softmax'ta exp(x) hÄ±zla patlar! Ã‡Ã¶zÃ¼m: exp(x - max(x)). Bu deÄŸer deÄŸiÅŸtirmez Ã§Ã¼nkÃ¼ exp(a-c)/Î£exp(b-c) = exp(a)/Î£exp(b).",
        highlight: "Flash Attention'Ä±n numerik stabilitesi bu trick'e dayanÄ±r. microGPT'de de math.exp(x - max_x) kullanÄ±lÄ±r!",
        code: "# YANLIÅ â€” overflow riski:\ndef naive_softmax(x):\n    return [math.exp(xi) / sum(math.exp(xj) for xj in x) for xi in x]\n\n# DOÄRU â€” numerik stabil:\ndef safe_softmax(x):\n    max_x = max(x)  # max Ã§Ä±kar\n    exps = [math.exp(xi - max_x) for xi in x]\n    total = sum(exps)\n    return [e / total for e in exps]\n\n# microGPT satÄ±r 142 â€” tam olarak bunu yapar!"
      },
      {
        title: { tr: "KontrollÃ¼ Deney TasarÄ±mÄ± â€” Ablation Study", en: "Controlled Experiment Design â€” Ablation Study" },
        viz: "ablationDesign",
        content: "Ablation = bir bileÅŸeni Ã§Ä±karÄ±p etkisini Ã¶lÃ§. Kontrol deÄŸiÅŸkeni: her seferinde SADECE 1 ÅŸey deÄŸiÅŸir. Seed sabitle, 3+ tekrar yap, standart sapma raporla.",
        highlight: "KÃ¶tÃ¼ deney: 'n_embd=32 ve n_layer=2 denedik, iyi oldu.' Ä°yi deney: 'n_embd=16â†’32 (diÄŸerleri sabit): loss 2.31â†’2.18 (Â±0.04, n=5)'",
        code: "# Sistematik ablation framework:\nimport json, statistics\n\ndef run_ablation(base_config, param, values, seeds=[42,123,456]):\n    results = {}\n    for val in values:\n        config = {**base_config, param: val}\n        losses = [train_and_eval(config, seed=s) for s in seeds]\n        results[val] = {\n            'mean': statistics.mean(losses),\n            'std': statistics.stdev(losses),\n            'n': len(seeds)\n        }\n    return results  # â†’ tablo ve grafik iÃ§in"
      },
      {
        title: { tr: "Akademik Rapor YapÄ±sÄ± â€” Related Work", en: "Academic Report Structure â€” Related Work" },
        content: "AraÅŸtÄ±rma makalesi yapÄ±sÄ±: Abstract â†’ Introduction â†’ Related Work â†’ Method â†’ Experiments â†’ Results â†’ Discussion â†’ Conclusion. Related Work: alanÄ± tanÄ±, boÅŸluÄŸu gÃ¶ster, katkÄ±nÄ± konumla.",
        highlight: "Related Work = 'baÅŸkalarÄ± X yaptÄ±, biz Y'yi farklÄ± yapÄ±yoruz Ã§Ã¼nkÃ¼ Z'. Her iddia citation ile desteklenmeli.",
        code: "# Makale yapÄ±sÄ± kontrol listesi:\npaper_structure = {\n    'abstract': '100-300 kelime, baÄŸÄ±msÄ±z Ã¶zet',\n    'introduction': 'Motivasyon â†’ Problem â†’ KatkÄ± â†’ Yol haritasÄ±',\n    'related_work': 'AlanÄ± tara â†’ BoÅŸluÄŸu gÃ¶ster â†’ Konumlan',\n    'method': 'Tekrarlanabilir detay + formÃ¼ller',\n    'experiments': 'Veri + Metrik + Baseline + Ablation',\n    'results': 'Tablo + Grafik + Ä°statistik',\n    'discussion': 'Limitasyon + Gelecek iÅŸ',\n    'references': 'BibTeX, tutarlÄ± format'\n}"
      },
      {
        title: { tr: "ğŸ§ª Ä°leri Lab â€” microGPT Ablation Deneyi", en: "ğŸ§ª Advanced Lab â€” microGPT Ablation Experiment" },
        content: "GerÃ§ek bir ablation deneyi tasarlayÄ±n: n_embd âˆˆ {8, 16, 32, 64} iÃ§in loss karÅŸÄ±laÅŸtÄ±rmasÄ±. 3 seed ile tekrar, std raporla. SonuÃ§larÄ± tablo ve grafikle sunun.",
        highlight: "Bu lab bir YL tezinin deney bÃ¶lÃ¼mÃ¼nÃ¼ simÃ¼le eder. SonuÃ§larÄ±nÄ±zÄ± academic format'ta raporlayÄ±n.",
        code: "# microGPT ablation komutu:\nfor embd in 8 16 32 64; do\n  for seed in 42 123 456; do\n    python3 microgpt.py --n_embd $embd --seed $seed \\\n      --num_steps 1000 > results/embd${embd}_s${seed}.log\n  done\ndone\n\n# SonuÃ§larÄ± parse et:\nimport glob, re\nfor f in sorted(glob.glob('results/*.log')):\n    final_loss = float(re.findall(r'loss ([\d.]+)', open(f).read())[-1])\n    print(f'{f}: {final_loss:.4f}')"
      }
    ]
  },

  {
    id: "frontiers", week: 9, title: { tr: "AraÅŸtÄ±rma SÄ±nÄ±rlarÄ± & YL Proje Rehberi", en: "Research Frontiers & Graduate Project Guide" }, icon: "ğŸ“", color: "#7C3AED",
    subtitle: { tr: "NAS, distillation, RoPE, sparse attention, grokking, flat minima, proje planlama", en: "NAS, distillation, RoPE, sparse attention, grokking, flat minima, project planning" },
    sections: [
      {
        title: { tr: "Neural Architecture Search (NAS) â€” Pareto FrontÄ±", en: "Neural Architecture Search (NAS) â€” Pareto Front" },
        viz: "nasPareto",
        content: "NAS = mimariyi de Ã¶ÄŸren! Arama uzayÄ±: katman sayÄ±sÄ±, head sayÄ±sÄ±, n_embd, MLP boyutu. Pareto frontÄ±: performans vs maliyet trade-off'unda optimal noktalar kÃ¼mesi.",
        highlight: "Pareto-optimal: A'dan daha iyi B yoktur hem performans HEM maliyet aÃ§Ä±sÄ±ndan. Karar vericiye bÄ±rakÄ±lÄ±r: hÄ±z mÄ± kalite mi?",
        code: "# NAS arama uzayÄ± (microGPT):\nsearch_space = {\n    'n_embd': [8, 16, 32, 64, 128],\n    'n_layer': [1, 2, 4, 6],\n    'n_head': [1, 2, 4],\n    'lr': [1e-2, 3e-3, 1e-3],\n}\n\n# Grid search â†’ Pareto frontÄ± bul:\nresults = []\nfor config in product(*search_space.values()):\n    loss = train_eval(config)\n    params = count_params(config)\n    results.append((params, loss, config))\n\n# Pareto filter:\npareto = [r for r in results\n    if not any(r2[0]<=r[0] and r2[1]<r[1] for r2 in results)]"
      },
      {
        title: { tr: "Knowledge Distillation â€” BÃ¼yÃ¼kten KÃ¼Ã§Ã¼ÄŸe Transfer", en: "Knowledge Distillation â€” Large to Small Transfer" },
        viz: "distillationFlow",
        content: "Teacher (bÃ¼yÃ¼k model) soft probability daÄŸÄ±lÄ±mÄ±nÄ± student'a Ã¶ÄŸretir. YÃ¼ksek T: daÄŸÄ±lÄ±m yumuÅŸak â†’ 'yanlÄ±ÅŸ' cevaplardan bile bilgi akÄ±ÅŸÄ±. Loss = Î±Â·CE_hard + (1-Î±)Â·KL_soft.",
        highlight: "T=1'de student sadece doÄŸru cevabÄ± Ã¶ÄŸrenir. T=5'te 'neredeyse doÄŸru' alternatifleri de Ã¶ÄŸrenir â†’ daha zengin bilgi.",
        code: "# Distillation loss:\ndef distill_loss(student_logits, teacher_logits, labels, T=4, alpha=0.7):\n    # Hard loss (normal cross-entropy)\n    hard = F.cross_entropy(student_logits, labels)\n    \n    # Soft loss (teacher'dan Ã¶ÄŸren)\n    soft_student = F.log_softmax(student_logits / T, dim=-1)\n    soft_teacher = F.softmax(teacher_logits / T, dim=-1)\n    soft = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T**2)\n    \n    return alpha * soft + (1 - alpha) * hard"
      },
      {
        title: { tr: "RoPE â€” Rotary Position Embedding", en: "RoPE â€” Rotary Position Embedding" },
        viz: "ropeViz",
        content: "Learned PE: her pozisyon sabit vektÃ¶r (context dÄ±ÅŸÄ±na genellemez). RoPE: vektÃ¶rÃ¼ pozisyona gÃ¶re DÃ–NDÃœR â†’ gÃ¶receli mesafe doÄŸal olarak kodlanÄ±r. cos/sin rotasyon matrisi.",
        highlight: "RoPE'un bÃ¼yÃ¼sÃ¼: q_m Â· k_n sadece (m-n) farkÄ±na baÄŸlÄ± â†’ mesafe bilgisi Ã§arpma iÃ§inde gÃ¶mÃ¼lÃ¼. Extrapolation bedava!",
        code: "# RoPE implementasyonu:\ndef apply_rope(x, pos):\n    d = x.shape[-1]\n    freqs = 1.0 / (10000 ** (torch.arange(0, d, 2) / d))\n    angles = pos.unsqueeze(-1) * freqs  # [T, d/2]\n    cos_a, sin_a = angles.cos(), angles.sin()\n    \n    x1, x2 = x[..., ::2], x[..., 1::2]  # Ã§ift/tek\n    # 2D rotasyon: [cos -sin; sin cos] Ã— [x1; x2]\n    return torch.cat([\n        x1 * cos_a - x2 * sin_a,\n        x1 * sin_a + x2 * cos_a\n    ], dim=-1)\n\n# microGPT: learned PE â†’ RoPE upgrade\n# wpe(pos) yerine apply_rope(q, pos), apply_rope(k, pos)"
      },
      {
        title: { tr: "Sparse Attention â€” O(nÂ²) â†’ O(nâˆšn)", en: "Sparse Attention â€” O(nÂ²) â†’ O(nâˆšn)" },
        viz: "sparseAttention",
        content: "Full attention: her token herkese bakar â†’ O(nÂ²). Sparse: sadece lokal pencere + global tokenlar â†’ O(nâˆšn). %50 sparsity â‰ˆ %50 FLOPs tasarrufu ama kalite kaybÄ± minimal.",
        highlight: "Longformer: lokal + global. BigBird: lokal + global + random. Mistral: sliding window. Flash Attention: sparse deÄŸil ama IO-optimal.",
        code: "# Sparse attention mask patterns:\ndef local_mask(seq_len, window=256):\n    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n    for i in range(seq_len):\n        start = max(0, i - window)\n        mask[i, start:i+1] = True\n    return mask\n\ndef global_local_mask(seq_len, window=256, n_global=4):\n    mask = local_mask(seq_len, window)\n    mask[:, :n_global] = True   # ilk n token global\n    mask[:n_global, :] = True\n    return mask\n\n# FLOPs karÅŸÄ±laÅŸtÄ±rma:\n# Full: 2 Ã— nÂ² Ã— d = 2 Ã— 1024Â² Ã— 64 = 134M\n# Sparse (w=256): â‰ˆ 2 Ã— n Ã— w Ã— d = 33M (75% â†“)"
      },
      {
        title: { tr: "Grokking Fenomeni â€” GeÃ§ Genelleme", en: "Grokking Phenomenon â€” Delayed Generalization" },
        viz: "grokkingViz",
        content: "Garip olay: eÄŸitim loss'u Ã§oktan 0 olmuÅŸken, test loss'u BÄ°NLERCE epoch sonra aniden dÃ¼ÅŸer! Memorize â†’ generalize geÃ§iÅŸi. Weight decay ve regularization tetikleyici.",
        highlight: "Grokking = 'aha anÄ±'. Model Ã¶nce ezberleri, sonra yapÄ±yÄ± keÅŸfeder. Erken durdurma (early stopping) grokking'i kaÃ§Ä±rabilir!",
        code: "# Grokking deneyi (modular arithmetic):\n# Veri: (a + b) mod 97 = c\nimport random\nN = 97\ndata = [(a, b, (a+b) % N) for a in range(N) for b in range(N)]\nrandom.shuffle(data)\ntrain = data[:len(data)//2]\ntest  = data[len(data)//2:]\n\n# EÄŸitim: ~300 epoch'ta train_loss â†’ 0\n#         ~3000 epoch'ta test_loss â†’ 0 (GROKKING!)\n# Anahtar: weight_decay=0.01 olmadan grokking yok"
      },
      {
        title: { tr: "Loss Landscape â€” Flat vs Sharp Minima", en: "Loss Landscape â€” Flat vs Sharp Minima" },
        viz: "lossLandscape",
        content: "Flat minimum = geniÅŸ vadi, kÃ¼Ã§Ã¼k perturbasyonlara dayanÄ±klÄ± â†’ iyi genelleme. Sharp minimum = dar Ã§ukur, hafif kayma = bÃ¼yÃ¼k loss artÄ±ÅŸÄ± â†’ kÃ¶tÃ¼ genelleme.",
        highlight: "SAM optimizer: 'en kÃ¶tÃ¼ komÅŸuda bile iyi ol' â†’ flat minima arar. Large batch = sharp, small batch = flat.",
        code: "# Sharpness-Aware Minimization (SAM):\ndef sam_step(model, loss_fn, data, rho=0.05):\n    # 1. Normal gradient hesapla\n    loss = loss_fn(model(data))\n    loss.backward()\n    \n    # 2. En kÃ¶tÃ¼ komÅŸuya git (perturbation)\n    with torch.no_grad():\n        for p in model.parameters():\n            e = rho * p.grad / p.grad.norm()\n            p.add_(e)  # worst-case neighbor\n    \n    # 3. O noktada gradient hesapla\n    loss2 = loss_fn(model(data))\n    loss2.backward()\n    \n    # 4. Geri dÃ¶n ve SAM gradient ile gÃ¼ncelle\n    with torch.no_grad():\n        for p in model.parameters():\n            p.sub_(e)  # geri dÃ¶n\n    optimizer.step()  # SAM gradient"
      },
      {
        title: { tr: "Ablation Study â€” Sistematik Deney Rehberi", en: "Ablation Study â€” Systematic Experiment Guide" },
        content: "Her YL projesinde ablation zorunlu. AmaÃ§: her bileÅŸenin katkÄ±sÄ±nÄ± kanÄ±tla. Template: Full model â†’ -component A â†’ -component B â†’ ... En az 3 seed, p-value raporla.",
        highlight: "Ä°yi ablation: 'Attention head sayÄ±sÄ± 4â†’2'ye dÃ¼ÅŸÃ¼nce loss %3.2Â±0.4 arttÄ± (p<0.05).' KÃ¶tÃ¼ ablation: 'Attention iyi Ã§alÄ±ÅŸÄ±yor.'",
        code: "# Ablation tablo template:\n# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n# â”‚ KonfigÃ¼rasyon     â”‚ Loss   â”‚ Params â”‚ p-value â”‚\n# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n# â”‚ Full model       â”‚ 2.18   â”‚ 3,648  â”‚ â€”       â”‚\n# â”‚ âˆ’ Multi-head     â”‚ 2.31   â”‚ 3,520  â”‚ 0.003   â”‚\n# â”‚ âˆ’ Layer norm     â”‚ 2.42   â”‚ 3,616  â”‚ 0.001   â”‚\n# â”‚ âˆ’ Residual       â”‚ 2.67   â”‚ 3,648  â”‚ <0.001  â”‚\n# â”‚ âˆ’ Embedding dim/2â”‚ 2.45   â”‚ 1,024  â”‚ 0.008   â”‚\n# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n# p-value hesaplama:\nfrom scipy import stats\nt_stat, p_val = stats.ttest_ind(full_losses, ablated_losses)"
      },
      {
        title: { tr: "ğŸ“ YL Proje Yol HaritasÄ± â€” BaÅŸlangÄ±Ã§tan Savunmaya", en: "ğŸ“ Graduate Project Roadmap â€” From Start to Defense" },
        content: "12 haftalÄ±k plan: H1-2: LiteratÃ¼r taramasÄ± + araÅŸtÄ±rma sorusu. H3-4: Baseline implementasyon. H5-8: Deneyler + ablation. H9-10: YazÄ±m. H11: Review. H12: Savunma hazÄ±rlÄ±k.",
        highlight: "AltÄ±n kural: Her hafta 1 tablo/grafik Ã¼ret. 12 hafta = 12 sonuÃ§. Tez kendiliÄŸinden yazÄ±lÄ±r.",
        code: "# YL Proje kontrol listesi:\nproject_plan = {\n    'week_1_2': 'LiteratÃ¼r: 20+ makale oku, RW yaz',\n    'week_3_4': 'Baseline: microGPT Ã§alÄ±ÅŸtÄ±r, metrik belirle',\n    'week_5_6': 'Deney 1: Ana hipotezi test et',\n    'week_7_8': 'Deney 2: Ablation + karÅŸÄ±laÅŸtÄ±rma',\n    'week_9': 'Grafik ve tablo hazÄ±rla',\n    'week_10': 'YazÄ±m: Method + Experiments',\n    'week_11': 'Peer review + revizyon',\n    'week_12': 'Sunum hazÄ±rla + prova'\n}\n\n# Her deney iÃ§in kayÄ±t:\nexperiment_log = {\n    'date': '2025-01-15',\n    'config': {'n_embd': 32, 'n_layer': 2},\n    'seed': [42, 123, 456],\n    'result': {'mean_loss': 2.18, 'std': 0.04},\n    'notes': 'Residual baÄŸlantÄ± kritik'\n}"
      }
    ]
  },

  {
    id: "paper", week: "B",
    title: "Attention Is All You Need",
    subtitle: { tr: "Vaswani et al. 2017 â€” Transformer makalesinin interaktif keÅŸfi", en: "Vaswani et al. 2017 â€” Interactive exploration of the Transformer paper" },
    color: "#6366F1",
    icon: "ğŸ“„",
    sections: [
      { title: { tr: "Transformer Nedir?", en: "What is a Transformer?" }, content: "2017'de Google araÅŸtÄ±rmacÄ±larÄ± RNN ve CNN'leri tamamen atÄ±p sadece attention kullanan Transformer modelini yaptÄ±. Hem daha iyi sonuÃ§ hem Ã§ok daha hÄ±zlÄ±.", highlight: "Eski yÃ¶ntemde (RNN) model her sÃ¶zcÃ¼ÄŸÃ¼ SIRAYLA iÅŸler. Transformer TÃœM sÃ¶zcÃ¼klere aynÄ± anda bakar.", viz: "tePaperGiris" },
      { title: { tr: "Eski Modellerin SorunlarÄ±", en: "Problems with Old Models" }, content: "RNN sÄ±ralÄ± Ã§alÄ±ÅŸÄ±r â†’ paralel olamaz â†’ yavaÅŸ. Uzun cÃ¼mlelerde erken sÃ¶zcÃ¼klerin bilgisi kaybolur. Gradient patlamasÄ±/sÃ¶nmesi yaÅŸanÄ±r.", highlight: "RNN: O(n) adÄ±m, Transformer: O(1) adÄ±m. Herkes herkesi gÃ¶rÃ¼r!", viz: "tePaperEskiMod" },
      { title: { tr: "Attention MekanizmasÄ±", en: "Attention Mechanism" }, content: "Query: Ne arÄ±yorum? Key: Bende ne var? Value: Ä°ÅŸte bilgim. QÂ·K yÃ¼ksekse â†’ o kelimenin Value'sundan Ã§ok bilgi al!", highlight: "KÃ¼tÃ¼phane analojisi: Query = aradÄ±ÄŸÄ±nÄ±z konu, Key = kitap etiketi, Value = kitabÄ±n iÃ§eriÄŸi.", viz: "tePaperAttention" },
      { title: { tr: "Matematik: Softmax, Dot Product, Multi-Head", en: "Math: Softmax, Dot Product, Multi-Head" }, content: "3 temel formÃ¼l: â‘  Dot Product â‘¡ Softmax â‘¢ Scaled Dot-Product Attention. Her birini kaydÄ±rÄ±cÄ±larla keÅŸfedin.", highlight: "Attention(Q,K,V) = softmax(QKáµ€/âˆšd)V â€” makalenin en Ã¼nlÃ¼ formÃ¼lÃ¼.", viz: "tePaperMat" },
      { title: { tr: "Mimari: Encoder-Decoder", en: "Architecture: Encoder-Decoder" }, content: "Encoder girdi cÃ¼mlesini anlar (6 katman). Decoder Ã§Ä±ktÄ± cÃ¼mlesini Ã¼retir (6 katman). Her katmanda: Attention + FFN + Residual + LayerNorm.", highlight: "Decoder'da causal mask: gelecek kelimeleri gÃ¶remez!", viz: "tePaperMimari" },
      { title: { tr: "Pozisyon Kodlama", en: "Positional Encoding" }, content: "Transformer sÄ±ra bilmez! sin/cos dalgalarÄ±yla her pozisyona benzersiz bir 'parmak izi' eklenir. FarklÄ± frekanslar farklÄ± Ã¶lÃ§eklerde kalÄ±p yakalar.", highlight: "DÃ¼ÅŸÃ¼k boyutlar hÄ±zlÄ± deÄŸiÅŸir (tiz), yÃ¼ksek boyutlar yavaÅŸ deÄŸiÅŸir (bas) â€” piyano gibi!", viz: "tePaperPoz" },
      { title: { tr: "EÄŸitim ve SonuÃ§lar", en: "Training and Results" }, content: "4.5M cÃ¼mle Ã§ifti, 8Ã— P100 GPU, 3.5 gÃ¼n. ENâ†’DE: 28.4 BLEU (rekor!). ENâ†’FR: 41.8 BLEU. Warmup + label smoothing + dropout.", highlight: "Base model: 65M parametre. Big model: 213M parametre. BugÃ¼nkÃ¼ GPT-4: ~1T+ parametre!", viz: "tePaperEgitim" },
      { title: { tr: "DÃ¼nyayÄ± NasÄ±l DeÄŸiÅŸtirdi?", en: "How Did It Change the World?" }, content: "90K+ atÄ±f! GPT, BERT, ViT, DALL-E, AlphaFold, Copilot â€” hepsi Transformer tabanlÄ±. 15 sayfalÄ±k makale tÃ¼m AI'Ä± deÄŸiÅŸtirdi.", highlight: "Sadece dil deÄŸil: gÃ¶rÃ¼ntÃ¼ (ViT), protein (AlphaFold), mÃ¼zik (MusicGen), kod (Copilot).", viz: "tePaperEtki" },
    ]
  },
];

export { WEEKS };
