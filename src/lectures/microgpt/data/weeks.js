// â”€â”€â”€ Lecture Content â€” All Weeks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const WEEKS = [
  {
    id: "intro", week: 0, title: { tr: "GiriÅŸ & CanlÄ± Demo", en: "Introduction & Live Demo" }, icon: "ğŸš€", color: "#0EA5E9",
    subtitle: { tr: "microGPT nedir, neden sÄ±fÄ±rdan, kurulum, Ã§alÄ±ÅŸtÄ±rma, sonuÃ§larÄ± gÃ¶zlemleme", en: "What is microGPT, why from scratch, setup, running, observing results" },
    sections: [
      {
        title: { tr: "Derse HoÅŸ Geldiniz â€” Ne Ã–ÄŸreneceksiniz?", en: "Welcome â€” What You Will Learn" },
        viz: "coursePipeline",
        content: "243 satÄ±r saf Python ile tam bir GPT â€” satÄ±r satÄ±r anlayacaksÄ±nÄ±z. HiÃ§bir dÄ±ÅŸ kÃ¼tÃ¼phane yok. YukarÄ±daki pipeline'Ä±n her aÅŸamasÄ±nÄ± ayrÄ± bir hafta iÅŸleyeceÄŸiz.",
        highlight: "\"This file is the complete algorithm. Everything else is just efficiency.\" â€” Andrej Karpathy"
      },
      {
        title: { tr: "Ã–n Bilgi: Yapay Sinir AÄŸÄ± Nedir?", en: "Background: What is a Neural Network?" },
        viz: "neuralNetBasics",
        content: "Sinir aÄŸÄ± = Ã¶ÄŸrenilebilir parametreli bir fonksiyon. 3 sekmeyi keÅŸfedin: ğŸ”¬ NÃ¶ron'da kaydÄ±rÄ±cÄ±larla aÄŸÄ±rlÄ±klarÄ± deÄŸiÅŸtirip Ã§Ä±ktÄ±yÄ± canlÄ± gÃ¶rÃ¼n. ğŸŒŠ Veri AkÄ±ÅŸÄ±'nda verinin nÃ¶rondan nasÄ±l geÃ§tiÄŸini adÄ±m adÄ±m izleyin. ğŸ¯ Mini EÄŸitim'de modelin ev fiyatÄ±nÄ± tahmin etmeyi nasÄ±l Ã¶ÄŸrendiÄŸini simÃ¼le edin.",
        highlight: "EÄŸitim = bu kaydÄ±rÄ±cÄ±larÄ± (wâ‚, wâ‚‚, b) veriye gÃ¶re OTOMATÄ°K ayarlama. GPT'de 3,648 tane var!"
      },
      {
        title: { tr: "Ã–n Bilgi: Dil Modeli Nedir?", en: "Background: What is a Language Model?" },
        viz: "langModelConcept",
        content: "Dil modeli = 'sonraki token ne olabilir?' sorusuna cevap veren olasÄ±lÄ±k makinesi. 3 sekmeyi keÅŸfedin: ğŸ² Autoregressive Ãœretim'de 'emma' isminin harf harf nasÄ±l Ã¼retildiÄŸini izleyin. ğŸ“± Telefon Analojisi'nde harf yazarak autocomplete'in GPT ile aynÄ± mantÄ±kta Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n. ğŸ“š 'emma' EÄŸitimi'nde her harf Ã§iftinin olasÄ±lÄ±ÄŸÄ±nÄ± eÄŸitim Ã¶ncesi/sonrasÄ± karÅŸÄ±laÅŸtÄ±rÄ±n.",
        highlight: "GPT = telefon autocompletenin Ã§ok bÃ¼yÃ¼k Ã¶lÃ§ekli hali. AynÄ± mantÄ±k: Ã¶nceki tokenlara bakarak sonrakini tahmin et."
      },
      {
        title: { tr: "Ne YapÄ±yor Bu Kod? â€” CanlÄ± Pipeline", en: "What Does This Code Do? â€” Live Pipeline" },
        viz: "livePipeline",
        content: "YukarÄ±daki animasyonda â–¶ butonuna basÄ±n â€” 'emma' isminin 9 aÅŸamalÄ± yolculuÄŸunu adÄ±m adÄ±m izleyin. Her kutuya tÄ±klayarak o aÅŸamanÄ±n detayÄ±nÄ± gÃ¶rebilirsiniz.",
        code: `# Tek komutla Ã§alÄ±ÅŸtÄ±rÄ±n:
$ python3 microgpt.py
# vocab size: 28, num params: 3648
# step 1/1000 | loss 3.33 â† rastgele
# step 1000   | loss 2.00 â† Ã¶ÄŸrendi!
# sample 0: kamrin â† yeni, gerÃ§ekÃ§i isim!`,
        highlight: "Her kutuya tÄ±klayarak o aÅŸamanÄ±n detayÄ±nÄ± gÃ¶rebilirsiniz"
      },
      {
        title: { tr: "Neden SÄ±fÄ±rdan? Framework KarÅŸÄ±laÅŸtÄ±rma", en: "Why From Scratch? Framework Comparison" },
        viz: "frameworkCompare",
        content: "YukarÄ±daki panelde 'ArkasÄ±nda ne var?' butonuna tÄ±klayÄ±n â€” PyTorch'un 3 satÄ±rÄ±nÄ±n arkasÄ±nda neler gizlendiÄŸini gÃ¶rÃ¼n. microgpt.py'de aynÄ± iÅŸlem aÃ§Ä±kÃ§a yazÄ±lmÄ±ÅŸ.",
        highlight: "Framework = verimlilik aracÄ±, algoritma deÄŸil. Ã–nce algoritmayÄ± anlayÄ±n, sonra framework hÄ±zlandÄ±rsÄ±n."
      },
      {
        title: { tr: "Ã–n KoÅŸullar & Kurulum", en: "Prerequisites & Setup" },
        content: "Tek gereksinim: Python 3.6+. pip install gerekmez â€” sadece os, math, random kullanÄ±lÄ±r.",
        code: `# Python kontrol:
$ python3 --version  # 3.6+ yeterli

# pip install GEREKMÄ°YOR!
import os      # dosya kontrolÃ¼
import math    # log, exp
import random  # rastgele sayÄ±lar`,
        highlight: "pip install gerekmez â€” sadece os, math, random kullanÄ±lÄ±r"
      },
      {
        title: { tr: "Kodu Ä°ndirme & Ä°lk Ã‡alÄ±ÅŸtÄ±rma", en: "Download & First Run" },
        content: "GitHub Gist'ten tek dosya indirin ve Ã§alÄ±ÅŸtÄ±rÄ±n. Loss dÃ¼ÅŸÃ¼yorsa her ÅŸey doÄŸru!",
        code: `# Ä°ndir:
$ curl -o microgpt.py https://gist.githubusercontent.com/karpathy/.../microgpt.py

# Ã‡alÄ±ÅŸtÄ±r (hÄ±zlÄ± test):
$ python3 microgpt.py --num_steps 200

# Loss dÃ¼ÅŸÃ¼yorsa â†’ model Ã¶ÄŸreniyor âœ“`,
        highlight: "Loss dÃ¼ÅŸÃ¼yorsa her ÅŸey doÄŸru!"
      },
      {
        title: { tr: "7 Temel Parametre â€” Ä°nteraktif Lab", en: "7 Core Parameters â€” Interactive Lab" },
        viz: "paramDist",
        content: "AÅŸaÄŸÄ±daki kaydÄ±rÄ±cÄ±larla parametreleri deÄŸiÅŸtirin â€” parametre sayÄ±sÄ±, bellek ve komut satÄ±rÄ± komutu canlÄ± gÃ¼ncellenir. n_embd artÄ±nca parametreler KARESEL bÃ¼yÃ¼r!",
        code: `# VarsayÄ±lan:
$ python3 microgpt.py  # 3,648 param

# BÃ¼yÃ¼k model:
$ python3 microgpt.py --n_embd 32 --n_layer 2
# â†’ ~14K param (4Ã— artÄ±ÅŸ!)`,
        highlight: "n_embd artÄ±nca parametreler KARESEL bÃ¼yÃ¼r!"
      },
      {
        title: { tr: "Kendi Verinizi Kullanma", en: "Using Your Own Data" },
        content: "input.txt'i deÄŸiÅŸtirerek TÃ¼rkÃ§e isimler, ÅŸehir adlarÄ± veya hayvan isimleri Ã¶ÄŸretebilirsiniz. Vocab otomatik hesaplanÄ±r.",
        code: `# TÃ¼rkÃ§e isim verisi:
$ cat > input.txt << EOF
ahmet
mehmet
ayse
fatma
zeynep
EOF
$ python3 microgpt.py --num_steps 500
# SonuÃ§: mehet, aysun, fatem...`,
        highlight: "Vocab otomatik hesaplanÄ±r"
      },
      {
        title: { tr: "Ãœretim Evrimi â€” CanlÄ± SimÃ¼lasyon", en: "Generation Evolution â€” Live Simulation" },
        viz: "trainingEvolution",
        content: "â–¶ BaÅŸlat'a basÄ±n â€” eÄŸitim boyunca loss'un dÃ¼ÅŸÃ¼ÅŸÃ¼nÃ¼ ve Ã¼retilen isimlerin kalite artÄ±ÅŸÄ±nÄ± canlÄ± izleyin. KaydÄ±rÄ±cÄ± ile istediÄŸiniz aÅŸamaya zÄ±playabilirsiniz.",
        highlight: "AdÄ±m 1: 'xqwpzml' (rastgele) â†’ AdÄ±m 1000: 'ellora' (gerÃ§ekÃ§i). AynÄ± 3,648 parametre â€” sadece eÄŸitim!"
      },
      {
        title: { tr: "GPT Ailesi â€” Ã–lÃ§ek Kulesi", en: "GPT Family â€” Scale Tower" },
        viz: "gptScaleTower",
        content: "Kulelerin Ã¼zerine gelin â€” her modelin detaylarÄ±nÄ± gÃ¶rÃ¼n. microGPT bir bisiklet, GPT-4 bir uzay mekiÄŸi â€” ama aynÄ± fizik kurallarÄ± geÃ§erli.",
        highlight: "Bu kodu anlarsanÄ±z GPT-4'Ã¼n %90'Ä±nÄ± anlarsÄ±nÄ±z. Kalan %10: RoPE, GQA, SwiGLU, MoE."
      }
    ]
  },

  {
    id: "tokenization", week: 1, title: { tr: "Tokenization & Embedding", en: "Tokenization & Embedding" }, icon: "ğŸ”¤", color: "#8B5CF6",
    subtitle: { tr: "Metni sayÄ±lara, sayÄ±larÄ± vektÃ¶rlere Ã§evirme â€” modelin dÃ¼nyayÄ± gÃ¶rme biÃ§imi", en: "Converting text to numbers, numbers to vectors â€” how the model sees the world" },
    sections: [
      {
        title: { tr: "Ã–n Bilgi: Bilgisayarlar Metni NasÄ±l Ä°ÅŸler?", en: "Background: How Do Computers Process Text?" },
        viz: "tokenFlow",
        content: "Bilgisayarlar metin iÅŸleyemez â€” her ÅŸey sayÄ±sal olmalÄ±dÄ±r. Tokenization = metni modelin anlayacaÄŸÄ± ID dizisine Ã§evirme. Bu kodda her karakter = bir token.",
        highlight: "Tokenization = modelin 'gÃ¶zlÃ¼ÄŸÃ¼'. FarklÄ± tokenizer = farklÄ± dÃ¼nya gÃ¶rÃ¼ÅŸÃ¼."
      },
      {
        title: { tr: "Temel TanÄ±mlar â€” Token, Vocab, Logit", en: "Key Definitions â€” Token, Vocab, Logit" },
        viz: "neuralNetBasics",
        content: "Token = modelin iÅŸlediÄŸi en kÃ¼Ã§Ã¼k birim (bu kodda karakter). Vocabulary = tÃ¼m token kÃ¼mesi (a-z + BOS = 27). Logit = modelin ham Ã§Ä±ktÄ± skorlarÄ± (softmax Ã¶ncesi).",
        highlight: "Metin â†’ sayÄ± â†’ vektÃ¶r dÃ¶nÃ¼ÅŸÃ¼mÃ¼ olmadan model hiÃ§bir ÅŸey yapamaz."
      },
      {
        title: { tr: "ğŸ® Tokenizer Oyun AlanÄ± â€” CanlÄ± SimÃ¼lasyon", en: "ğŸ® Tokenizer Playground â€” Live Simulation" },
        viz: "tokenizerPlayground",
        content: "YukarÄ±ya bir isim yazÄ±n ve â–¶ butonuna basÄ±n â€” tokenization'Ä±n 4 adÄ±mÄ±nÄ± canlÄ± izleyin: karakterlere ayrÄ±lma â†’ ID'lere Ã§evrilme â†’ BOS eklenmesi â†’ eÄŸitim Ã§iftlerinin oluÅŸmasÄ±.",
        code: `# Vocabulary oluÅŸturma:
uchars = sorted(set(''.join(docs)))  # ['a'...'z']
BOS = len(uchars)  # = 26 (Ã¶zel token)

# Tokenize etme:
tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]
# "emma" â†’ [26, 4, 12, 12, 0, 26]`,
        highlight: "YukarÄ±ya bir isim yazÄ±n ve â–¶ butonuna basÄ±n â€” tokenization'Ä±n 4 adÄ±mÄ±nÄ± canlÄ± iz"
      },
      {
        title: { tr: "Ã–n Bilgi: VektÃ¶r Nedir? Neden KullanÄ±rÄ±z?", en: "Background: What is a Vector? Why Use Them?" },
        viz: "vectorConcept",
        content: "VektÃ¶r = sÄ±ralÄ± sayÄ±lar listesi. Benzer ÅŸeyler â†’ yakÄ±n vektÃ¶rler. Bu kodda her token 16 boyutlu bir vektÃ¶rle temsil edilir.",
        highlight: "Embedding = token'Ä± vektÃ¶re Ã§evirme. EÄŸitim sonunda benzer tokenlar yakÄ±n vektÃ¶rlere sahip olur."
      },
      {
        title: { tr: "Token Embedding: ID â†’ VektÃ¶r DÃ¶nÃ¼ÅŸÃ¼mÃ¼", en: "Token Embedding: ID â†’ Vector Transform" },
        viz: "embeddingFlow",
        content: "Embedding tablosu [27Ã—16]. Ä°ÅŸlem basit tablo arama: wte[token_id] â†’ 16-boyutlu vektÃ¶r. BaÅŸlangÄ±Ã§ta rastgele, eÄŸitimle anlam kazanÄ±r.",
        code: `tok_emb = state_dict['wte'][token_id]
# 'a' â†’ ID=0 â†’ wte[0] = [0.02, -0.01, ...]
# EÄŸitim sonrasÄ±: sesli harfler yakÄ±n vektÃ¶rlerde`,
        highlight: "BaÅŸlangÄ±Ã§ta rastgele, eÄŸitimle anlam kazanÄ±r"
      },
      {
        title: { tr: "Position Embedding: SÄ±ra Bilgisi Ekleme", en: "Position Embedding: Adding Order Information" },
        viz: "embeddingFlow",
        content: "Transformer sÄ±ra bilgisi Ä°Ã‡ERMEZ! Position embedding ile her konuma (0-7) Ã¶zgÃ¼ vektÃ¶r eklenir: x = tok_emb + pos_emb.",
        code: `pos_emb = state_dict['wpe'][pos_id]
x = [t + p for t, p in zip(tok_emb, pos_emb)]
# AynÄ± 'a' â†’ pos 0 ve pos 3'te FARKLI temsil`,
        highlight: "Transformer sÄ±ra bilgisi Ä°Ã‡ERMEZ! Position embedding ile her konuma (0-7) Ã¶zgÃ¼ v"
      },
      {
        title: { tr: "Ã–n Bilgi: Matris Ã‡arpÄ±mÄ± (Linear Transform)", en: "Background: Matrix Multiplication (Linear Transform)" },
        viz: "matrixMul",
        content: "Her Ã§Ä±ktÄ± = girdi vektÃ¶rÃ¼ ile aÄŸÄ±rlÄ±k satÄ±rÄ±nÄ±n dot product'Ä±. Transformer'da HER projeksiyon (Wq, Wk, Wv, fc1, fc2) bir matris Ã§arpÄ±mÄ±.",
        code: `def linear(x, weight):
    return [sum(wi*xi for wi,xi in zip(wo,x))
            for wo in weight]
# Wq [16Ã—16] Ã— x [16] = q [16]  (256 Ã§arpma)`,
        highlight: "Transformer'da HER projeksiyon (Wq, Wk, Wv, fc1, fc2) bir matris Ã§arpÄ±mÄ±"
      },
      {
        title: { tr: "Weight Tying & Parametre DaÄŸÄ±lÄ±mÄ±", en: "Weight Tying & Parameter Distribution" },
        viz: "paramDist",
        content: "AynÄ± wte matrisi hem giriÅŸ (lookup) hem Ã§Ä±kÄ±ÅŸ (matris Ã§arpÄ±mÄ±) iÃ§in kullanÄ±lÄ±r â€” weight tying. 3,648 parametrenin %56'sÄ± MLP'de!",
        code: `# GÄ°RÄ°Å: tok_emb = wte[token_id]     # lookup
# Ã‡IKIÅ: logits = linear(x, wte)      # matris Ã§arpÄ±mÄ±
# AynÄ± matris! â†’ 432 param tasarrufu`,
        highlight: "3,648 parametrenin %56'sÄ± MLP'de!"
      },
      {
        title: { tr: "Softmax â€” Skorlardan OlasÄ±lÄ±klara", en: "Softmax â€” From Scores to Probabilities" },
        viz: "softmaxViz",
        content: "Softmax ham skorlarÄ± olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na Ã§evirir: P(i) = exp(xi)/Î£exp(xj). Numerik trick: exp(xi-max) ile overflow Ã¶nlenir.",
        code: `def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    return [e / sum(exps) for e in exps]
# [2.0, 1.0, 0.1] â†’ [0.659, 0.242, 0.099]`,
        highlight: "Numerik trick: exp(xi-max) ile overflow Ã¶nlenir"
      }
    ]
  },

  {
    id: "autograd", week: 2, title: { tr: "Autograd & Backpropagation", en: "Autograd & Backpropagation" }, icon: "â›“ï¸", color: "#F59E0B",
    subtitle: { tr: "Otomatik tÃ¼rev alma, hesaplama grafÄ±, chain rule â€” derin Ã¶ÄŸrenmenin temeli", en: "Automatic differentiation, computation graph, chain rule â€” the foundation of deep learning" },
    sections: [
      {
        title: { tr: "Ã–n Bilgi: TÃ¼rev Nedir? Neden LazÄ±m?", en: "Background: What is a Derivative? Why Do We Need It?" },
        viz: "derivative",
        content: "TÃ¼rev = deÄŸiÅŸim hÄ±zÄ±. f(x) = xÂ² ise f'(3) = 6. Derin Ã¶ÄŸrenmede: âˆ‚L/âˆ‚w = 'w'yi deÄŸiÅŸtirirsem loss ne olur?' Negatif yÃ¶nde gÃ¼ncelle â†’ loss azalÄ±r.",
        highlight: "TÃ¼rev = 'bu parametreyi hangi yÃ¶nde deÄŸiÅŸtirmeliyim ki loss azalsÄ±n?' sorusunun cevabÄ±."
      },
      {
        title: { tr: "Ã–n Bilgi: KÄ±smi TÃ¼rev ve Gradient", en: "Background: Partial Derivatives and Gradients" },
        viz: "derivative",
        content: "f(a,b) = aÃ—b ise âˆ‚f/âˆ‚a = b, âˆ‚f/âˆ‚b = a (diÄŸerini sabit tut). Gradient = tÃ¼m kÄ±smi tÃ¼revlerin vektÃ¶rÃ¼. Gradient descent = gradient'in ters yÃ¶nÃ¼nde adÄ±m at.",
        code: `# f(a,b) = aÃ—b + aÂ²  (a=2, b=3 â†’ f=10)
# âˆ‚f/âˆ‚a = b + 2a = 7  (a'yÄ± 1â†‘ â†’ fâ‰ˆ7â†‘)
# âˆ‚f/âˆ‚b = a = 2        (b'yi 1â†‘ â†’ fâ‰ˆ2â†‘)
# Gradient: âˆ‡f = [7, 2]
# Minimum: a -= lrÃ—7, b -= lrÃ—2`,
        highlight: "Gradient descent = gradient'in ters yÃ¶nÃ¼nde adÄ±m at"
      },
      {
        title: { tr: "ğŸ® Autograd Oyun AlanÄ± â€” CanlÄ± Backward Pass", en: "ğŸ® Autograd Playground â€” Live Backward Pass" },
        viz: "autogradPlayground",
        content: "YukarÄ±da a, b, c deÄŸerlerini kaydÄ±rÄ±cÄ±larla deÄŸiÅŸtirin ve â–¶ Backward butonuna basÄ±n â€” chain rule adÄ±mlarÄ±nÄ±n hesaplama grafÄ± Ã¼zerinde nasÄ±l ilerlediÄŸini canlÄ± izleyin!",
        highlight: "Autograd = derin Ã¶ÄŸrenmeyi mÃ¼mkÃ¼n kÄ±lan teknoloji. Bu kodda ~30 satÄ±rla sÄ±fÄ±rdan yazÄ±lmÄ±ÅŸtÄ±r."
      },
      {
        title: { tr: "Value SÄ±nÄ±fÄ± â€” 4 Temel BileÅŸen", en: "Value Class â€” 4 Core Components" },
        viz: "opGradTable",
        content: "Her sayÄ± Value nesnesi olarak sarmalanÄ±r: data (deÄŸer), grad (tÃ¼rev, baÅŸta 0), _children (girdi dÃ¼ÄŸÃ¼mleri), _local_grads (yerel tÃ¼revler).",
        code: `class Value:
    __slots__ = ('data','grad','_children','_local_grads')
    def __init__(self, data, children=(), local_grads=()):
        self.data = data; self.grad = 0
        self._children = children
        self._local_grads = local_grads`,
        highlight: "Her sayÄ± Value nesnesi olarak sarmalanÄ±r: data (deÄŸer), grad (tÃ¼rev, baÅŸta 0), _"
      },
      {
        title: { tr: "OperatÃ¶r Overloading â€” Otomatik Graf OluÅŸturma", en: "Operator Overloading â€” Automatic Graph Building" },
        viz: "opGradTable",
        content: "a+b, a*b gibi iÅŸlemler otomatik graf oluÅŸturur. Her operasyon yerel gradient'ini bilir: toplamaâ†’(1,1), Ã§arpmaâ†’(b,a), ReLUâ†’(a>0?1:0).",
        code: `def __mul__(self, other):
    other = other if isinstance(other, Value) else Value(other)
    return Value(self.data * other.data,
        (self, other), (other.data, self.data))
# âˆ‚(aÃ—b)/âˆ‚a = b, âˆ‚(aÃ—b)/âˆ‚b = a`,
        highlight: "Her operasyon yerel gradient'ini bilir: toplamaâ†’(1,1), Ã§arpmaâ†’(b,a), ReLUâ†’(a>0?1:0)"
      },
      {
        title: { tr: "Chain Rule & Backward Pass", en: "Chain Rule & Backward Pass" },
        viz: "compGraph",
        content: "Chain rule: f(g(x)) tÃ¼revi = f'(g(x)) Ã— g'(x). TÃ¼revler geri doÄŸru Ã‡ARPILIR. Topological sort ile doÄŸru sÄ±rada, self.grad=1'den baÅŸlayarak hesaplanÄ±r.",
        code: `def backward(self):
    topo = [];
 visited = set()
    def build_topo(v):
        if v not in visited:
            visited.add(v)
            for child in v._children: build_topo(child)
            topo.append(v)
    build_topo(self)
    self.grad = 1  # âˆ‚L/âˆ‚L = 1
    for v in reversed(topo):
        for child, lg in zip(v._children, v._local_grads):
            child.grad += lg * v.grad  # chain rule!`,
        highlight: "Topological sort ile doÄŸru sÄ±rada, self.grad=1'den baÅŸlayarak hesaplanÄ±r"
      },
      {
        title: "Somut Ã–rnek: L = (a Ã— b) + c",
        viz: "compGraph",
        content: "a=2, b=3, c=1 â†’ d=6, L=7. Backward: âˆ‚L/âˆ‚L=1, âˆ‚L/âˆ‚d=1, âˆ‚L/âˆ‚c=1, âˆ‚L/âˆ‚a=b=3, âˆ‚L/âˆ‚b=a=2. Autograd oyun alanÄ±nda bizzat deneyin!",
        highlight: "a.grad=3 â†’ a'yÄ± 1 birim artÄ±rÄ±rsak loss 3 birim artar. Optimizer bu bilgiyi kullanÄ±r."
      },
      {
        title: "Gradient ToplanmasÄ± (+=) Neden Kritik?",
        viz: "compGraph",
        content: "L = aÃ—a ise âˆ‚L/âˆ‚a = 2a. += ile iki yoldan gelen gradient toplanÄ±r â†’ doÄŸru. = ile sadece son yol kalÄ±r â†’ yanlÄ±ÅŸ! Weight tying, residual'da bu durum sÃ¼rekli olur.",
        code: `a = Value(3); L = a * a  # a Ä°KÄ° KEZ kullanÄ±lÄ±r
# += ile: a.grad = 3 + 3 = 6 = 2a  âœ“
# =  ile: a.grad = 3  â† YANLIÅ! (2a=6 olmalÄ±)`,
        highlight: "= ile sadece son yol kalÄ±r â†’ yanlÄ±ÅŸ! Weight tying, residual'da bu durum sÃ¼rekli olur"
      },
      {
        title: "Bu Kod vs. PyTorch KarÅŸÄ±laÅŸtÄ±rma",
        viz: "opGradTable",
        content: "Birebir aynÄ± gradient deÄŸerleri! Fark: Value=skaler (~30 satÄ±r Python), Tensor=N-boyutlu (~100K+ satÄ±r C++/CUDA, GPU ile milyonlarca kat hÄ±zlÄ±).",
        highlight: "Her eÄŸitim adÄ±mÄ±nda p.grad = 0 yapÄ±lmazsa gradient birikir â†’ model patlar!"
      }
    ]
  },

  {
    id: "attention", week: 3, title: { tr: "Self-Attention MekanizmasÄ±", en: "Self-Attention Mechanism" }, icon: "ğŸ”", color: "#10B981",
    subtitle: { tr: "QÂ·Káµ€/âˆšd â†’ Softmax â†’ V â€” Transformer'Ä±n kalbi", en: "QÂ·Káµ€/âˆšd â†’ Softmax â†’ V â€” The heart of the Transformer" },
    sections: [
      {
        title: "Ã–n Bilgi: RNN'den Attention'a",
        viz: "rnnToAttn",
        content: "2017 Ã¶ncesi RNN/LSTM standardÄ±: sÄ±ralÄ±, yavaÅŸ, uzun mesafe baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± unutur. 'Attention Is All You Need' â†’ RNN'yi kaldÄ±r, SADECE attention kullan = Transformer.",
        highlight: "Transformer = Attention + Feed-Forward, RNN yok. Bu basit fikir tÃ¼m modern AI'Ä± mÃ¼mkÃ¼n kÄ±ldÄ±."
      },
      {
        title: "Self-Attention â€” Sezgisel Anlama",
        viz: "attentionFlow",
        content: "Her token Ã¶nceki tokenlara bakarak bilgi toplar. BazÄ±larÄ±na Ã§ok dikkat eder (yÃ¼ksek aÄŸÄ±rlÄ±k), bazÄ±larÄ±nÄ± yok sayar. Bu aÄŸÄ±rlÄ±klar dinamik â€” her girdi iÃ§in yeniden hesaplanÄ±r.",
        highlight: "Attention = dinamik, veri-baÄŸÄ±mlÄ± aÄŸÄ±rlÄ±klama. Statik deÄŸil â€” her girdi iÃ§in farklÄ±."
      },
      {
        title: "Query, Key, Value â€” 3 FarklÄ± Rol",
        viz: "attentionFlow",
        content: "Q = 'ne arÄ±yorum?', K = 'bende ne var?', V = 'bilgim nedir'. QÂ·K dot product = uyum skoru. YÃ¼ksek skor â†’ V'den daha Ã§ok bilgi al.",
        code: `q = linear(x, state_dict['attn_wq'])  # [16]â†’[16]
k = linear(x, state_dict['attn_wk'])
v = linear(x, state_dict['attn_wv'])
# QÂ·K yÃ¼ksek â†’ o token'a Ã§ok dikkat et`,
        highlight: "YÃ¼ksek skor â†’ V'den daha Ã§ok bilgi al"
      },
      {
        title: "ğŸ® Attention Oyun AlanÄ± â€” Head KalÄ±plarÄ±nÄ± KeÅŸfet",
        viz: "attentionPlayground",
        content: "YukarÄ±da 4 farklÄ± head seÃ§ip 'Banana' kelimesinde her token'Ä±n nelere dikkat ettiÄŸini gÃ¶rÃ¼n. SatÄ±rlara tÄ±klayarak dikkat daÄŸÄ±lÄ±mÄ±nÄ± inceleyin â€” her head farklÄ± kalÄ±p Ã¶ÄŸrenir!",
        highlight: "Her head baÄŸÄ±msÄ±z attention hesabÄ± yapar. Biri sesli-sessiz uyumunu, diÄŸeri pozisyon yakÄ±nlÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenebilir."
      },
      {
        title: "Scaled Dot-Product â€” Tam Hesaplama",
        viz: "attentionFlow",
        content: "4 adÄ±m: QÂ·K (skor) â†’ Ã·âˆšd (scaling) â†’ softmax (olasÄ±lÄ±k) â†’ Ã—V (bilgi toplama). âˆšd bÃ¶lme kritik: boyut bÃ¼yÃ¼yÃ¼nce softmax spike yapar, gradient kaybolur.",
        code: `for t in range(len(keys)):
    score[t] = dot(q, keys[t]) / sqrt(head_dim)
weights = softmax(scores)
out = weighted_sum(weights, values)
# Attention(Q,K,V) = softmax(QÂ·Káµ€/âˆšd)Â·V`,
        highlight: "âˆšd bÃ¶lme kritik: boyut bÃ¼yÃ¼yÃ¼nce softmax spike yapar, gradient kaybolur"
      },
      {
        title: "Ã–n Bilgi: Dot Product â€” Benzerlik Ã–lÃ§Ã¼mÃ¼",
        viz: "dotProduct",
        content: "aÂ·b = Î£ aáµ¢Ã—báµ¢. AynÄ± yÃ¶n â†’ pozitif (benzer), dik â†’ 0 (ilgisiz), ters â†’ negatif (zÄ±t). QÂ·K = sorgu ile anahtar ne kadar uyumlu?",
        code: `# qÂ·k1 = 0.2+0.06+0.02+0.03 = 0.31 (benzer!)
# qÂ·k2 = -0.25-0.09-0.04-0.01 = -0.39 (zÄ±t)
# â†’ q, k1'e daha Ã§ok dikkat edecek`,
        highlight: "QÂ·K = sorgu ile anahtar ne kadar uyumlu?"
      },
      {
        title: "Multi-Head & Causal Masking",
        viz: "causalMask",
        content: "16 boyut â†’ 4 head Ã— 4 dim. Her head baÄŸÄ±msÄ±z attention yapar, sonra birleÅŸtirilip Wo ile projekte edilir. Causal mask: her token sadece Ã–NCEKÄ°LERÄ° gÃ¶rÃ¼r.",
        code: `# Multi-head: her head 4 boyutluk dilim
for h in range(4):
    q_h = q[h*4:(h+1)*4]
    # baÄŸÄ±msÄ±z attention â†’ concatenate â†’ Wo
# Causal: KV cache = doÄŸal mask`,
        highlight: "Causal mask: her token sadece Ã–NCEKÄ°LERÄ° gÃ¶rÃ¼r"
      },
      {
        title: "Attention Ã‡Ä±ktÄ±sÄ± & Linear Projeksiyon",
        viz: "residualViz",
        content: "4 head birleÅŸtirilir (4Ã—4=16), Wo matrisi ile projekte edilir, residual eklenir: x = WoÃ—heads + x_residual. Wo sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity.",
        code: `# 6 linear projeksiyon: Wq, Wk, Wv, Wo, fc1, fc2
# Hepsi aynÄ±: x Ã— W (bias yok, modern trend)`,
        highlight: "Wo sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity"
      }
    ]
  },

  {
    id: "transformer", week: 4, title: { tr: "Transformer BloklarÄ±", en: "Transformer Blocks" }, icon: "ğŸ§±", color: "#EC4899",
    subtitle: { tr: "RMSNorm, MLP, Residual â€” tam mimari, katman katman", en: "RMSNorm, MLP, Residual â€” full architecture, layer by layer" },
    sections: [
      {
        title: "Genel Mimari â€” BÃ¼yÃ¼k Resim",
        viz: "archPipeline",
        content: "Bir Transformer katmanÄ± = Attention + MLP. Her blok Ã¶ncesi RMSNorm, sonrasÄ± residual. AkÄ±ÅŸ: x â†’ norm â†’ attn â†’ +x â†’ norm â†’ MLP â†’ +x â†’ Ã§Ä±ktÄ±.",
        highlight: "Pre-norm: norm â†’ block â†’ +residual. Modern modellerin standardÄ± â€” daha kararlÄ± eÄŸitim."
      },
      {
        title: "ğŸ® Transformer BloÄŸu â€” AdÄ±m AdÄ±m SimÃ¼lasyon",
        viz: "transformerBlockFlow",
        content: "â–¶ AkÄ±ÅŸ butonuna basÄ±n ve 8 aÅŸamayÄ± sÄ±rayla izleyin: girdi â†’ RMSNorm â†’ Attention â†’ +Residual â†’ RMSNorm â†’ MLP â†’ +Residual â†’ Ã§Ä±ktÄ±. Kutulara tÄ±klayarak adÄ±m atlayÄ±n.",
        highlight: "Her aÅŸama bir iÅŸlev: Norm=kararlÄ±lÄ±k, Attn=bilgi toplama, MLP=bilgi iÅŸleme, Residual=gradient highway."
      },
      {
        title: "RMSNorm â€” NasÄ±l Ã‡alÄ±ÅŸÄ±r?",
        viz: "normCompare",
        content: "RMS = âˆšmean(xÂ²). Her elemanÄ± RMS'e bÃ¶l â†’ normalize. LayerNorm'dan farkÄ±: ortalama Ã§Ä±karmaz â†’ %30 daha hÄ±zlÄ±, eÅŸdeÄŸer kalite.",
        code: `def rmsnorm(x):
    ms = sum(xi*xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]`,
        highlight: "LayerNorm'dan farkÄ±: ortalama Ã§Ä±karmaz â†’ %30 daha hÄ±zlÄ±, eÅŸdeÄŸer kalite"
      },
      {
        title: "MLP Bloku â€” Feed-Forward Network",
        viz: "mlpFlow",
        content: "Her token'Ä± baÄŸÄ±msÄ±z iÅŸler: geniÅŸlet (16â†’64) â†’ ReLUÂ² aktive â†’ daralt (64â†’16). ~%40 nÃ¶ron 'Ã¶lÃ¼' kalÄ±r (sparse = iyi!).",
        code: `h = linear(x, state_dict['mlp_fc1'])  # 16â†’64
h = [xi.relu()**2 for xi in h]        # ReLUÂ²
x = linear(h, state_dict['mlp_fc2'])  # 64â†’16
x = [a+b for a,b in zip(x, x_res)]   # +residual`,
        highlight: "~%40 nÃ¶ron 'Ã¶lÃ¼' kalÄ±r (sparse = iyi!)"
      },
      {
        title: "Ã–n Bilgi: Aktivasyon Fonksiyonu Neden Gerekli?",
        viz: "activation",
        content: "Aktivasyon olmadan derin aÄŸ = tek matris Ã§arpÄ±mÄ± (Wâ‚ƒÃ—Wâ‚‚Ã—Wâ‚Ã—x = WÃ—x). Non-linearity her katmana farklÄ± karar sÄ±nÄ±rÄ± Ã¶ÄŸretir.",
        highlight: "Aktivasyon = non-linearity. Onsuz derin aÄŸ = basit matris Ã§arpÄ±mÄ±. TÃ¼m gÃ¼Ã§ buradan gelir."
      },
      {
        title: "Residual BaÄŸlantÄ±lar â€” Gradient Highway",
        viz: "residualViz",
        content: "x = f(x) + x. Gradient doÄŸrudan giriÅŸe akar: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out Ã— (âˆ‚f/âˆ‚x + 1). +1 terimi = kestirme yol. Wo ve fc2 sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity.",
        code: `# Attention: x = attention(norm(x)) + x_res
# MLP:       x = mlp(norm(x)) + x_res
# Gradient: +1 terimi gradient'in kaybolmasÄ±nÄ± Ã¶nler`,
        highlight: "Wo ve fc2 sÄ±fÄ±rda baÅŸlar â†’ baÅŸta identity"
      },
      {
        title: "Weight Initialization â€” Kritik BaÅŸlatma",
        viz: "weightInit",
        content: "Genel parametreler: std=0.08 â‰ˆ 1/âˆšn_embd. Wo ve fc2: std=0 â†’ baÅŸta residual = identity. Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ kaybolma.",
        code: `# Genel: random.gauss(0, 0.08)
# Wo, fc2: random.gauss(0, 0.0)  â† sÄ±fÄ±ra yakÄ±n
# â†’ BaÅŸta: x â‰ˆ x + 0 = x (identity)`,
        highlight: "Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ kaybolma"
      },
      {
        title: "RMSNorm vs LayerNorm â€” KarÅŸÄ±laÅŸtÄ±rma",
        viz: "normCompare",
        content: "LayerNorm: Î¼ Ã§Ä±kar, Ïƒ'ya bÃ¶l, Î³ ve Î² uygula (4 iÅŸlem). RMSNorm: sadece RMS'e bÃ¶l, Î³ uygula (2 iÅŸlem). LLaMA, Mistral, Gemma hep RMSNorm.",
        highlight: "Ortalama Ã§Ä±karmak gereksiz bulundu â€” %30 hÄ±z kazancÄ±, sÄ±fÄ±r kalite kaybÄ±."
      }
    ]
  },

  {
    id: "training", week: 5, title: { tr: "EÄŸitim DÃ¶ngÃ¼sÃ¼", en: "Training Loop" }, icon: "ğŸ”„", color: "#EF4444",
    subtitle: { tr: "Loss, optimizer, learning rate â€” modeli Ã¶ÄŸretme sanatÄ±", en: "Loss, optimizer, learning rate â€” the art of teaching a model" },
    sections: [
      {
        title: "Ã–n Bilgi: Optimizasyon Nedir?",
        viz: "gradDescent",
        content: "Optimizasyon = loss fonksiyonunun minimumunu bulma. 3,648 boyutlu uzayda en alÃ§ak noktayÄ± arÄ±yoruz. Gradient descent = her adÄ±mda gradient'in ters yÃ¶nÃ¼nde kÃ¼Ã§Ã¼k adÄ±m.",
        highlight: "Derin Ã¶ÄŸrenme = fonksiyon optimizasyonu. Loss'u minimize eden parametreleri bulmak = tÃ¼m iÅŸ."
      },
      {
        title: "Gradient Descent â€” Sezgisel Anlama",
        viz: "gradDescent",
        content: "GÃ¶zleri kapalÄ± daÄŸda: en dik yokuÅŸu hissedip (gradient) tersine adÄ±m at (gÃ¼ncelleme). AdÄ±m boyutu = learning rate. Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ yavaÅŸ.",
        code: `p.data -= learning_rate * p.grad
# grad>0 â†’ p azalt, grad<0 â†’ p artÄ±r, grad=0 â†’ minimum`,
        highlight: "Ã‡ok bÃ¼yÃ¼k â†’ patlama, Ã§ok kÃ¼Ã§Ã¼k â†’ yavaÅŸ"
      },
      {
        title: "ğŸ® EÄŸitim SimÃ¼lasyonu â€” LR Etkisini Dene",
        viz: "trainingSim",
        content: "YukarÄ±da learning rate kaydÄ±rÄ±cÄ±sÄ±nÄ± ayarlayÄ±p â–¶ EÄŸit butonuna basÄ±n. Loss eÄŸrisinin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶zlemleyin: Ã§ok yÃ¼ksek LR â†’ patlama, Ã§ok dÃ¼ÅŸÃ¼k â†’ yavaÅŸ Ã¶ÄŸrenme, doÄŸru LR â†’ gÃ¼zel dÃ¼ÅŸÃ¼ÅŸ!",
        highlight: "LR Ã§ok bÃ¼yÃ¼k â†’ diverge, Ã§ok kÃ¼Ã§Ã¼k â†’ Ã§ok yavaÅŸ. Ä°yi bÃ¶lge: 0.005-0.05 arasÄ±."
      },
      {
        title: "Cross-Entropy Loss",
        viz: "lossTable",
        content: "L = -log(P(doÄŸru_token)). P=1â†’L=0, P=1/27â†’Lâ‰ˆ3.33 (rastgele). EÄŸitimle loss dÃ¼ÅŸer: 3.33 â†’ 2.8 â†’ 2.0 â†’ 1.5.",
        code: `loss_t = -probs[target_id].log()
loss = (1/n) * sum(losses)  # ortalama
# BaÅŸlangÄ±Ã§ â‰ˆ 3.33, hedef < 2.0`,
        highlight: "EÄŸitimle loss dÃ¼ÅŸer: 3.33 â†’ 2.8 â†’ 2.0 â†’ 1.5"
      },
      {
        title: "Ã–n Bilgi: Log Fonksiyonu â€” Neden?",
        viz: "crossEntropyGraph",
        content: "-log(p): p=1â†’0, pâ†’0â†’âˆ. DÃ¼ÅŸÃ¼k olasÄ±lÄ±ÄŸa aÄŸÄ±r ceza. Ã‡arpÄ±mlarÄ± toplama Ã§evirir (numerik kararlÄ±lÄ±k). Bilgi teorisi: sÃ¼rpriz Ã¶lÃ§Ã¼sÃ¼.",
        highlight: "-log(p) = sÃ¼rpriz. Beklenmeyen olay â†’ yÃ¼ksek sÃ¼rpriz â†’ yÃ¼ksek loss. Model sÃ¼rprizi minimize eder."
      },
      {
        title: "Adam Optimizer â€” SGD'nin Evrimi",
        viz: "adamEvolution",
        content: "SGD sorunlarÄ±: tek lr, gÃ¼rÃ¼ltÃ¼ye duyarlÄ±. Adam = Momentum (yÃ¶n) + RMSprop (Ã¶lÃ§ek). Her parametre kendi adaptif lr'Ä±nÄ± alÄ±r. NLP standardÄ±.",
        code: `m[i] = 0.85*m[i] + 0.15*p.grad      # momentum
v[i] = 0.99*v[i] + 0.01*p.grad**2   # adaptif Ã¶lÃ§ek
p.data -= lr * m_hat / (v_hat**0.5 + 1e-8)
p.grad = 0  # â† KRÄ°TÄ°K sÄ±fÄ±rlama!`,
        highlight: "SGD sorunlarÄ±: tek lr, gÃ¼rÃ¼ltÃ¼ye duyarlÄ±"
      },
      {
        title: "Learning Rate Decay & EÄŸitim DÃ¶ngÃ¼sÃ¼",
        viz: "lrDecay",
        content: "Linear decay: lr_t = lr Ã— (1 - step/num_steps). Modern: warmup + cosine decay. Tam dÃ¶ngÃ¼: forward â†’ loss â†’ backward â†’ adam gÃ¼ncelle â†’ grad sÄ±fÄ±rla â†’ tekrarla.",
        code: `lr_t = learning_rate * (1 - step / num_steps)
# step=0: lr=0.01, step=500: 0.005, step=1000: 0.0`,
        highlight: "Tam dÃ¶ngÃ¼: forward â†’ loss â†’ backward â†’ adam gÃ¼ncelle â†’ grad sÄ±fÄ±rla â†’ tekrarla"
      },
      {
        title: "Gradient SÄ±fÄ±rlama â€” Neden Åart?",
        viz: "trainingCycle",
        content: "Backward += ile gradient biriktirir. SÄ±fÄ±rlanmazsa: 0.5 â†’ 0.8 â†’ 1.5 â†’ âˆ â†’ model patlar! Her adÄ±mda p.grad = 0 yapÄ±lmalÄ±.",
        code: `for i, p in enumerate(params):
    p.data -= lr_t * m_hat / (v_hat**0.5 + 1e-8)
    p.grad = 0  # â† BU SATIR OLMADAN MODEL PATLAR`,
        highlight: "SÄ±fÄ±rlanmazsa: 0.5 â†’ 0.8 â†’ 1.5 â†’ âˆ â†’ model patlar! Her adÄ±mda p.grad = 0 yapÄ±lmalÄ±"
      }
    ]
  },

  {
    id: "inference", week: 6, title: "Inference & Text Generation", icon: "âœ¨", color: "#6366F1",
    subtitle: { tr: "Autoregressive sampling, temperature, KV cache â€” modelin konuÅŸma zamanÄ±", en: "Autoregressive sampling, temperature, KV cache â€” when the model speaks" },
    sections: [
      {
        title: "EÄŸitim vs Inference â€” Fark Nedir?",
        viz: "inferenceTimeline",
        content: "EÄŸitim: forward + backward + gÃ¼ncelle (paralel, gradient gerekli). Inference: sadece forward (sÄ±ralÄ±, gradient yok â†’ hÄ±zlÄ±, az bellek). EÄŸitim = sÄ±nav Ã§alÄ±ÅŸmak, Inference = sÄ±nav vermek.",
        highlight: "Inference'da backward pass YOK â†’ daha az bellek, daha hÄ±zlÄ±. Ama autoregressive olduÄŸu iÃ§in sÄ±ralÄ±."
      },
      {
        title: "Autoregressive Generation â€” AdÄ±m AdÄ±m",
        viz: "inferenceTimeline",
        content: "BOS ile baÅŸla â†’ model Ã§alÄ±ÅŸtÄ±r â†’ 27 olasÄ±lÄ±k â†’ temperature Ã¶lÃ§ekle â†’ softmax â†’ Ã¶rnekle â†’ BOS/EOS ise DUR, deÄŸilse tekrarla. SonuÃ§: 'kamrin' gibi yeni isimler!",
        code: `token_id = BOS; sample = []
for pos_id in range(block_size):
    logits = gpt(token_id, pos_id, keys, values)
    probs = softmax([l/temperature for l in logits])
    token_id = random.choices(range(27), weights=probs)[0]
    if token_id == BOS: break
    sample.append(uchars[token_id])`,
        highlight: "SonuÃ§: 'kamrin' gibi yeni isimler!"
      },
      {
        title: "ğŸ® Ãœretim Oyun AlanÄ± â€” Temperature Etkisini Dene",
        viz: "generationPlayground",
        content: "YukarÄ±da temperature kaydÄ±rÄ±cÄ±sÄ±nÄ± ayarlayÄ±p â–¶ Ãœret butonuna basÄ±n. DÃ¼ÅŸÃ¼k T â†’ her zaman aynÄ± isim, yÃ¼ksek T â†’ kaotik harfler. Softmax daÄŸÄ±lÄ±m ÅŸeklinin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶zlemleyin!",
        highlight: "DÃ¼ÅŸÃ¼k T â†’ deterministik (tekrar), Tâ‰ˆ0.8 â†’ dengeli (yaratÄ±cÄ± ama gerÃ§ekÃ§i), yÃ¼ksek T â†’ kaotik."
      },
      {
        title: "Temperature Scaling â€” YaratÄ±cÄ±lÄ±k KontrolÃ¼",
        viz: "temperatureViz",
        content: "probs = softmax(logits/T). Tâ†’0: greedy (deterministik). T=0.8: dengeli. T=1: orijinal daÄŸÄ±lÄ±m. T>1: dÃ¼z (rastgele). Fizik analojisi: dÃ¼ÅŸÃ¼k T â†’ dÃ¼zenli, yÃ¼ksek T â†’ kaotik.",
        code: `# T=0.5: [0.876, 0.117, 0.007] â† sivri
# T=1.0: [0.659, 0.242, 0.099] â† dengeli
# T=2.0: [0.387, 0.337, 0.276] â† dÃ¼z`,
        highlight: "Fizik analojisi: dÃ¼ÅŸÃ¼k T â†’ dÃ¼zenli, yÃ¼ksek T â†’ kaotik"
      },
      {
        title: "Sampling Stratejileri",
        viz: "samplingViz2",
        content: "Bu kodda: random sampling (tam daÄŸÄ±lÄ±mdan). Alternatifler: Greedy (argmax), Top-k (en yÃ¼ksek k token), Top-p/Nucleus (kÃ¼mÃ¼latif olasÄ±lÄ±k eÅŸiÄŸi). Production: genelde top-p + temperature.",
        highlight: "Bu kod en basit stratejiyi kullanÄ±r. Production modeller genelde top-p (nucleus) tercih eder."
      },
      {
        title: "KV Cache â€” O(nÂ²) â†’ O(n)",
        viz: "kvCache",
        content: "Ã–nceki pozisyonlarÄ±n K,V vektÃ¶rlerini sakla. Her yeni token iÃ§in sadece 1 K,V hesapla, Ã¶ncekiler cache'ten oku. Bu kodda Python listesi, production'da Paged Attention.",
        code: `keys[layer_idx].append(k)    # cache'e ekle
values[layer_idx].append(v)
# pos=5: keys = [kâ‚€,...,kâ‚…] â† Ã¶ncekiler hazÄ±r!`,
        highlight: "Bu kodda Python listesi, production'da Paged Attention"
      },
      {
        title: "Inference Pipeline â€” UÃ§tan Uca Ã–rnek",
        viz: "inferenceTimeline",
        content: "'kamrin': BOSâ†’'k'(P=.08)â†’'a'(.15)â†’'m'(.11)â†’'r'(.09)â†’'i'(.22)â†’'n'(.18)â†’BOS(.31)â†’DUR. Veri setinde yok ama Ä°ngilizce yapÄ±sÄ±na uygun!",
        highlight: "Model olasÄ±lÄ±k Ã¼retir, sampling seÃ§er. FarklÄ± temperature/seed â†’ farklÄ± isimler."
      },
      {
        title: "Bu Kod vs Production GPT â€” KapanÄ±ÅŸ",
        viz: "whatsMissing",
        content: "Birebir aynÄ± algoritma! Fark sadece Ã¶lÃ§ek: 3,648 vs 1T+ parametre, 8 vs 128K+ context, dakikalar vs aylar eÄŸitim, $0 vs $100M+ maliyet. TEMELDEKÄ° MATEMATÄ°K AYNI. Tebrikler!",
        highlight: "243 satÄ±r Python ile GPT'nin tÃ¼m temellerini Ã¶ÄŸrendiniz. ArtÄ±k 'sihir' deÄŸil, anlaÅŸÄ±lÄ±r matematik!"
      }
    ]
  },

  {
    id: "evolution", week: 7, title: { tr: "Modern AI'a Evrim", en: "Evolution to Modern AI" }, icon: "ğŸŒ", color: "#14B8A6",
    subtitle: { tr: "243 satÄ±rdan ChatGPT'ye â€” Ã¶lÃ§ek, donanÄ±m, yazÄ±lÄ±m ve paradigma deÄŸiÅŸimleri", en: "From 243 lines to ChatGPT â€” scale, hardware, software, and paradigm shifts" },
    sections: [
      {
        title: "Scaling Laws â€” Daha BÃ¼yÃ¼k = Daha Ä°yi?",
        viz: "scalingLaws",
        content: "Kaplanick et al. (2020): loss âˆ 1/N^Î±. Parametre, veri ve hesaplama artÄ±nca loss dÃ¼ÅŸer â€” gÃ¼Ã§ yasasÄ± iliÅŸkisi. Chinchilla (2022): optimal oran = 20 token/parametre.",
        code: `# Scaling Law (Kaplanick et al. 2020):
# L(N) = a / N^b  (N=parametre sayÄ±sÄ±)
# microGPT:  N=3,648   â†’ loss â‰ˆ 2.0
# GPT-2:     N=1.5B    â†’ loss â‰ˆ 0.8
# GPT-3:     N=175B    â†’ loss â‰ˆ 0.5
# Chinchilla: optimal D/N â‰ˆ 20 (token/param)`,
        highlight: "microGPT'den GPT-4'e geÃ§iÅŸ 'yeni matematik' deÄŸil, AYNI matematiÄŸin 1 milyon kat bÃ¼yÃ¼tÃ¼lmesi."
      },
      {
        title: "ğŸ® Evrim Zaman Ã‡izelgesi â€” GPT-1'den BugÃ¼ne",
        viz: "evolutionTimeline",
        content: "AÅŸaÄŸÄ±daki zaman Ã§izelgesinde her modele tÄ±klayÄ±p parametre, veri, yenilik ve maliyet bilgilerini inceleyin. microGPT'den GPT-4'e 6 bÃ¼yÃ¼klÃ¼k mertebesi fark â€” ama temel aynÄ±!",
        highlight: "2018'den 2024'e: 117M â†’ 1.8T parametre, $10K â†’ $100M+ maliyet. Ama Transformer temeli hiÃ§ deÄŸiÅŸmedi."
      },
      {
        title: "DonanÄ±m Evrimi â€” CPU â†’ GPU â†’ TPU",
        viz: "hardwareEvolution",
        content: "CPU: sÄ±ralÄ±, genel amaÃ§lÄ±. GPU: binlerce kÃ¼Ã§Ã¼k Ã§ekirdek, paralel matris Ã§arpÄ±mÄ± (NVIDIA A100: 312 TFLOPS). TPU: Google'Ä±n Ã¶zel AI Ã§ipi. Yeni: Groq LPU, Cerebras WSE.",
        highlight: "GPU olmadan modern AI yok. A100 tek Ã§ipte 80GB bellek, 312 TFLOPS â€” microGPT'yi saniyede milyonlarca kez Ã§alÄ±ÅŸtÄ±rÄ±r."
      },
      {
        title: "EÄŸitim Pipeline'Ä± â€” Pre-training â†’ RLHF",
        viz: "trainingPipeline",
        content: "3 aÅŸama: (1) Pre-training: internet-Ã¶lÃ§eÄŸinde metin, next-token prediction â€” bu derste Ã¶ÄŸrendiÄŸiniz! (2) SFT: insan yazÄ±mÄ± soru-cevap ile fine-tune. (3) RLHF/DPO: insan tercihleri ile hizalama.",
        code: `# 3-AÅŸamalÄ± Modern LLM EÄŸitimi:
# 1. Pre-training (bu ders!):
#    loss = -log(P(next_token | prev_tokens))
#    Veri: internet (~13T token)
#    Maliyet: ~%95 toplam bÃ¼tÃ§e

# 2. SFT (Supervised Fine-Tuning):
#    (soru, cevap) Ã§iftleri ile fine-tune
#    ~100K Ã¶rnek

# 3. RLHF (Human Alignment):
#    reward_model = train(human_preferences)
#    policy = PPO(model, reward_model)`,
        highlight: "Pre-training = ham gÃ¼Ã§, SFT = yetenek, RLHF = 'iyi davranÄ±ÅŸ'. microGPT sadece adÄ±m 1'i yapÄ±yor."
      },
      {
        title: "Tokenization Evrimi â€” Karakter â†’ BPE â†’ SentencePiece",
        viz: "tokenEvolution",
        content: "microGPT: karakter dÃ¼zeyi (27 token). BPE (GPT-2): alt-kelime (~50K token, 'playing'â†’'play'+'ing'). SentencePiece (LLaMA): Unicode-aware. tiktoken (GPT-4): ~100K token.",
        highlight: "Daha iyi tokenizer = daha az token = daha uzun context = daha iyi anlama. Ama temel fikir aynÄ±: metinâ†’ID."
      },
      {
        title: "Attention Evrimi â€” Vanilla â†’ Flash â†’ Sliding Window",
        viz: "attentionEvolution",
        content: "Vanilla: O(nÂ²) bellek ve hesaplama. Multi-Query (2019): K,V paylaÅŸÄ±mÄ±. Flash Attention (2022): IO-aware â†’ 2-4Ã— hÄ±zlÄ±. Sliding Window (Mistral): sabit pencere â†’ âˆ context.",
        highlight: "Flash Attention = aynÄ± matematik, farklÄ± bellek eriÅŸim dÃ¼zeni. SonuÃ§ deÄŸiÅŸmez, hÄ±z 4Ã— artar."
      },
      {
        title: "AÃ§Ä±k Kaynak Devrimi â€” LLaMA, Mistral, DeepSeek",
        viz: "opensourceMap",
        content: "2023 kÄ±rÄ±lma noktasÄ±: LLaMA â†’ aÃ§Ä±k aÄŸÄ±rlÄ±klar, araÅŸtÄ±rma patlamasÄ±. Mistral 7B: kÃ¼Ã§Ã¼k ama gÃ¼Ã§lÃ¼. DeepSeek-V3: MoE, Ã¼cretsiz API. Qwen, Gemma, Command-R: Ã§eÅŸitlilik.",
        highlight: "AÃ§Ä±k kaynak modeller GPT-4 seviyesine yaklaÅŸÄ±yor. microGPT'nin prensipleri bunlarÄ±n HEPSÄ°NDE aynÄ±."
      },
      {
        title: "GÃ¼ncel Trendler â€” MoE, RAG, Agent, Multimodal",
        viz: "trendsRadar",
        content: "MoE: 8 uzman, her token 2 uzmanÄ± aktive eder â†’ verimlilik. RAG: dÄ±ÅŸ bilgi ile zenginleÅŸtirme. Agent: araÃ§ kullanÄ±mÄ± (kod, arama). Multimodal: metin+gÃ¶rÃ¼ntÃ¼+ses.",
        highlight: "Her trend, bu derste Ã¶ÄŸrendiÄŸiniz Transformer temelinin Ã¼stÃ¼ne inÅŸa edilir. Temel saÄŸlam = her ÅŸey mÃ¼mkÃ¼n."
      }
    ]
  },

  {
    id: "paper", week: "B",
    title: "Attention Is All You Need",
    subtitle: { tr: "Vaswani et al. 2017 â€” Transformer makalesinin interaktif keÅŸfi", en: "Vaswani et al. 2017 â€” Interactive exploration of the Transformer paper" },
    color: "#6366F1",
    icon: "ğŸ“„",
    sections: [
      { title: { tr: "Transformer Nedir?", en: "What is a Transformer?" }, content: "2017'de Google araÅŸtÄ±rmacÄ±larÄ± RNN ve CNN'leri tamamen atÄ±p sadece attention kullanan Transformer modelini yaptÄ±. Hem daha iyi sonuÃ§ hem Ã§ok daha hÄ±zlÄ±.", highlight: "Eski yÃ¶ntemde (RNN) model her sÃ¶zcÃ¼ÄŸÃ¼ SIRAYLA iÅŸler. Transformer TÃœM sÃ¶zcÃ¼klere aynÄ± anda bakar.", viz: "tePaperGiris" },
      { title: { tr: "Eski Modellerin SorunlarÄ±", en: "Problems with Old Models" }, content: "RNN sÄ±ralÄ± Ã§alÄ±ÅŸÄ±r â†’ paralel olamaz â†’ yavaÅŸ. Uzun cÃ¼mlelerde erken sÃ¶zcÃ¼klerin bilgisi kaybolur. Gradient patlamasÄ±/sÃ¶nmesi yaÅŸanÄ±r.", highlight: "RNN: O(n) adÄ±m, Transformer: O(1) adÄ±m. Herkes herkesi gÃ¶rÃ¼r!", viz: "tePaperEskiMod" },
      { title: { tr: "Attention MekanizmasÄ±", en: "Attention Mechanism" }, content: "Query: Ne arÄ±yorum? Key: Bende ne var? Value: Ä°ÅŸte bilgim. QÂ·K yÃ¼ksekse â†’ o kelimenin Value'sundan Ã§ok bilgi al!", highlight: "KÃ¼tÃ¼phane analojisi: Query = aradÄ±ÄŸÄ±nÄ±z konu, Key = kitap etiketi, Value = kitabÄ±n iÃ§eriÄŸi.", viz: "tePaperAttention" },
      { title: { tr: "Matematik: Softmax, Dot Product, Multi-Head", en: "Math: Softmax, Dot Product, Multi-Head" }, content: "3 temel formÃ¼l: â‘  Dot Product â‘¡ Softmax â‘¢ Scaled Dot-Product Attention. Her birini kaydÄ±rÄ±cÄ±larla keÅŸfedin.", highlight: "Attention(Q,K,V) = softmax(QKáµ€/âˆšd)V â€” makalenin en Ã¼nlÃ¼ formÃ¼lÃ¼.", viz: "tePaperMat" },
      { title: { tr: "Mimari: Encoder-Decoder", en: "Architecture: Encoder-Decoder" }, content: "Encoder girdi cÃ¼mlesini anlar (6 katman). Decoder Ã§Ä±ktÄ± cÃ¼mlesini Ã¼retir (6 katman). Her katmanda: Attention + FFN + Residual + LayerNorm.", highlight: "Decoder'da causal mask: gelecek kelimeleri gÃ¶remez!", viz: "tePaperMimari" },
      { title: { tr: "Pozisyon Kodlama", en: "Positional Encoding" }, content: "Transformer sÄ±ra bilmez! sin/cos dalgalarÄ±yla her pozisyona benzersiz bir 'parmak izi' eklenir. FarklÄ± frekanslar farklÄ± Ã¶lÃ§eklerde kalÄ±p yakalar.", highlight: "DÃ¼ÅŸÃ¼k boyutlar hÄ±zlÄ± deÄŸiÅŸir (tiz), yÃ¼ksek boyutlar yavaÅŸ deÄŸiÅŸir (bas) â€” piyano gibi!", viz: "tePaperPoz" },
      { title: { tr: "EÄŸitim ve SonuÃ§lar", en: "Training and Results" }, content: "4.5M cÃ¼mle Ã§ifti, 8Ã— P100 GPU, 3.5 gÃ¼n. ENâ†’DE: 28.4 BLEU (rekor!). ENâ†’FR: 41.8 BLEU. Warmup + label smoothing + dropout.", highlight: "Base model: 65M parametre. Big model: 213M parametre. BugÃ¼nkÃ¼ GPT-4: ~1T+ parametre!", viz: "tePaperEgitim" },
      { title: { tr: "DÃ¼nyayÄ± NasÄ±l DeÄŸiÅŸtirdi?", en: "How Did It Change the World?" }, content: "90K+ atÄ±f! GPT, BERT, ViT, DALL-E, AlphaFold, Copilot â€” hepsi Transformer tabanlÄ±. 15 sayfalÄ±k makale tÃ¼m AI'Ä± deÄŸiÅŸtirdi.", highlight: "Sadece dil deÄŸil: gÃ¶rÃ¼ntÃ¼ (ViT), protein (AlphaFold), mÃ¼zik (MusicGen), kod (Copilot).", viz: "tePaperEtki" },
    ]
  },
];

export { WEEKS };
