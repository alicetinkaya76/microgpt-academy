const EN_CONTENT = {
  intro: [
    { content: "A complete GPT in 243 lines of pure Python ‚Äî you will understand every single line. No external libraries. We will cover each stage of the pipeline above in separate weeks.", highlight: "\"This file is the complete algorithm. Everything else is just efficiency.\" ‚Äî Andrej Karpathy" },
    { content: "A neural network = a function with learnable parameters. Explore 3 tabs: üî¨ Neuron ‚Äî adjust weights with sliders and see the output live. üåä Data Flow ‚Äî follow data through a neuron step by step. üéØ Mini Training ‚Äî simulate how a model learns to predict house prices.", highlight: "Training = automatically adjusting these sliders (w‚ÇÅ, w‚ÇÇ, b) based on data. GPT has 3,648 of them!" },
    { content: "A language model = a probability machine that answers 'what could the next token be?' Explore 3 tabs: üé≤ Autoregressive Generation ‚Äî watch how 'emma' is generated letter by letter. üì± Phone Analogy ‚Äî see how phone autocomplete works on the same principle as GPT. üìö 'emma' Training ‚Äî compare pre/post training probabilities for each letter pair.", highlight: "GPT = a massively scaled-up version of phone autocomplete. Same logic: predict the next token based on previous ones." },
    { content: "Press ‚ñ∂ in the animation above ‚Äî follow the 9-step journey of generating 'emma' step by step. Click each box to see details of that stage.", highlight: "Click each box to see the details of that stage" },
    { content: "Click 'What\'s behind it?' in the panel above ‚Äî see what\'s hidden behind PyTorch\'s 3 lines. In microgpt.py, the same operation is written explicitly.", highlight: "Framework = efficiency tool, not the algorithm. Understand the algorithm first, then let the framework speed things up." },
    { content: "Only requirement: Python 3.6+. No pip install needed ‚Äî only os, math, random are used.", highlight: "No pip install needed ‚Äî only os, math, random are used" },
    { content: "Download a single file from GitHub Gist and run it. If loss is decreasing, everything is working!", highlight: "If loss is decreasing, everything is working!" },
    { content: "Change the parameters with sliders below ‚Äî parameter count, memory, and command line command update live. When n_embd increases, parameters grow QUADRATICALLY!", highlight: "When n_embd increases, parameters grow QUADRATICALLY!" },
    { content: "You can teach Turkish names, city names, or animal names by changing input.txt. Vocabulary is computed automatically.", highlight: "Vocabulary is computed automatically" },
    { content: "Loss start ~3.33 (random guessing among 27 tokens = -log(1/27)). If it drops below 2.0, the model has learned significant patterns. Generated names look realistic even if they\'re not in the training set.", highlight: "Loss < 2.0 means the model has learned! The generated names are new ‚Äî not memorized." },
  ],
  tokenization: [
    { content: "Computers only understand numbers. Tokenization = splitting text into pieces (tokens) and converting each to a number. microGPT uses character-level: each letter = one token. GPT-4 uses BPE: frequent word pieces become single tokens.", highlight: "microGPT: 'emma' ‚Üí ['e','m','m','a'] ‚Üí [4,12,12,0]. Character-level = simplest possible tokenizer." },
    { content: "Token, Vocab, BOS, EOS ‚Äî key concepts explained one by one. Token = smallest unit. Vocab = complete set of tokens. microGPT vocab = 27 (a-z + space).", highlight: "Vocab size directly affects model size. More tokens = more parameters = more expressive but harder to train." },
    { content: "Type characters below and watch how they get tokenized in real-time. Compare character-level tokenization with BPE. See how different tokenizers handle the same text differently.", highlight: "BPE learns frequent patterns: 'the' becomes one token instead of three." },
    { content: "A vector = a list of numbers describing something. One number isn\'t enough to describe a letter ‚Äî we need multiple dimensions. Like GPS: latitude alone isn\'t enough, you need longitude too.", highlight: "Embedding dimension (d=16 in microGPT) = how many numbers describe each token. More dimensions = richer description." },
    { content: "Embedding table [27√ó16]. Operation is simple table lookup: wte[token_id] ‚Üí 16-dimensional vector. Initially random, gains meaning through training.", highlight: "Initially random, gains meaning through training" },
    { content: "Transformer has no notion of order. Position embedding adds a unique 16-dimensional vector to each position. Without it, 'ab' and 'ba' would look identical.", highlight: "Without position embedding, the model cannot distinguish 'ab' from 'ba'!" },
    { content: "Matrix multiplication = the fundamental operation of neural networks. It transforms vectors from one space to another. Every layer in GPT is essentially a matrix multiplication.", highlight: "Matrix multiplication: the single most important operation in deep learning." },
    { content: "The same embedding matrix is used for both input and output. Input: token_id ‚Üí vector. Output: vector ‚Üí logits over vocabulary. This reduces parameters by half!", highlight: "Weight tying: same matrix for input and output = 50% fewer parameters in the embedding layer!" },
    { content: "Softmax converts raw scores (logits) to probabilities. Three properties: all values between 0-1, sum to 1, preserves order. Temperature parameter controls the sharpness of the distribution.", highlight: "Softmax = the bridge between raw model outputs and probabilities." },
  ],
  autograd: [
    { content: "Derivative = how fast does the output change when you slightly change the input? If f(x) = x¬≤, then f\'(x) = 2x. At x=3: rate of change = 6. This is the foundation of learning.", highlight: "Derivative is the compass of optimization: it tells us which direction to go and how big of a step to take." },
    { content: "With multiple inputs, we take the derivative with respect to each one separately. Gradient = collection of all partial derivatives. It points in the steepest ascent direction.", highlight: "Gradient = the steepest direction. Go opposite to minimize loss." },
    { content: "Build computation graphs, set values, and watch gradients flow backward in real-time. This is exactly what happens inside GPT during training.", highlight: "Every forward pass builds a graph. Backward pass computes gradients through this graph." },
    { content: "4 components: data (the number), grad (gradient, starts at 0), _backward (gradient computation function), _children (input nodes). Together they enable automatic differentiation.", highlight: "Value class = the atom of autograd. Everything builds on these 4 components." },
    { content: "When you write c = a + b, Python calls __add__. We override this to also build the computation graph. Result: math works normally AND the graph is built automatically.", highlight: "Operator overloading = the magic trick that makes autograd feel like normal math." },
    { content: "Chain rule: derivative of a composition = product of derivatives. f(g(x))\' = f\'(g(x)) √ó g\'(x). Backward pass applies this rule from output to input, through the entire graph.", highlight: "Chain rule is the mathematical foundation of backpropagation." },
    { content: "Topological sort ensures we process nodes in the right order. Gradient accumulation (+=, not =): when a variable is used multiple times, gradients ADD UP.", highlight: "Critical: grad += (not =!) ‚Äî Gradients ACCUMULATE." },
    { content: "Our Value class: Python, CPU, educational, ~50 lines. PyTorch Tensor: C++/CUDA, GPU, production, millions of lines. Same algorithm, vastly different scale.", highlight: "Same algorithm, vastly different scale. Understanding Value = understanding PyTorch internals." },
  ],
  attention: [
    { content: "RNN processes sequentially ‚Üí can\'t parallelize ‚Üí slow. Information from early words fades in long sentences. Attention: every word can directly look at every other word.", highlight: "RNN: O(n) steps, Transformer: O(1) steps. Everyone sees everyone!" },
    { content: "Dot product measures how similar two vectors are. Large positive = very similar, near zero = unrelated, large negative = opposite. This is how attention decides which tokens to focus on.", highlight: "Dot product = the similarity engine of attention." },
    { content: "Each token creates 3 vectors: Q (what am I looking for?), K (what do I contain?), V (here is my information). High Q¬∑K score ‚Üí take more of that token\'s V.", highlight: "Library analogy: Q = topic you\'re searching for, K = book label, V = book content." },
    { content: "Attention(Q,K,V) = softmax(QK·µÄ/‚àöd)V. The ‚àöd scaling prevents dot products from getting too large, which would make softmax too sharp (one-hot).", highlight: "Scaling by ‚àöd is crucial: without it, gradients vanish in softmax\'s flat regions." },
    { content: "Instead of one attention with d dimensions, use h heads each with d/h dimensions. Different heads learn different patterns: one might focus on position, another on phonetics.", highlight: "Multi-head = ensemble of specialists. Each head captures different relationships." },
    { content: "In generation, future tokens don\'t exist yet. Causal mask sets future attention scores to -‚àû ‚Üí softmax makes them 0. The model can only look backward.", highlight: "Causal mask: the fundamental constraint that makes autoregressive generation possible." },
  ],
  transformer: [
    { content: "RMSNorm normalizes the vector. ~30% faster than LayerNorm: no mean subtraction. Prevents gradient explosion by keeping values in a reasonable range.", highlight: "RMSNorm = LayerNorm minus the mean subtraction. Simpler and faster." },
    { content: "2-layer network: expand 4√ó then compress back. fc1: d‚Üí4d (expand information), ReGLU activation (filter), fc2: 4d‚Üíd (compress back). This is where the model stores knowledge.", highlight: "MLP = the model\'s knowledge store. Attention routes information, MLP processes it." },
    { content: "x = x + sublayer(x). Without residuals, gradients vanish in deep networks. Residual connections create a direct path for gradients to flow through.", highlight: "Residual connections = the information highway. They are what make deep networks trainable." },
    { content: "Input ‚Üí RMSNorm ‚Üí Self-Attention ‚Üí Residual ‚Üí RMSNorm ‚Üí MLP ‚Üí Residual ‚Üí Output. This is the complete transformer block. microGPT has exactly 1 such block.", highlight: "One block = Norm + Attention + Residual + Norm + MLP + Residual. Stack N of these = GPT." },
    { content: "Follow dimension changes through the entire model: token_id (scalar) ‚Üí embedding (d) ‚Üí attention Q,K,V (d) ‚Üí head split (d/h per head) ‚Üí merge (d) ‚Üí MLP (4d‚Üíd) ‚Üí logits (vocab).", highlight: "Understanding dimension flow = understanding the architecture." },
  ],
  training: [
    { content: "Cross-entropy measures surprise: if the model assigns high probability to the correct answer, loss is low. L = -log(P(target)). Starting loss ‚âà 3.33 = random guessing among 27 tokens.", highlight: "Low probability ‚Üí high surprise ‚Üí high loss. Training = reducing surprise." },
    { content: "Imagine a hilly landscape where height = loss. Gradient descent = always walk downhill. The gradient tells you the steepest direction. Step size = learning rate.", highlight: "Gradient descent: the optimization algorithm behind virtually all of deep learning." },
    { content: "Too large ‚Üí oscillate and diverge. Too small ‚Üí painfully slow. The sweet spot depends on the problem. Common trick: start large, decay over time (learning rate schedule).", highlight: "Learning rate is the single most important hyperparameter in training." },
    { content: "Adam = SGD + momentum + adaptive learning rate. Momentum: use past gradients for smoother updates. Adaptive: each parameter gets its own learning rate. This is what microGPT uses.", highlight: "Adam: the default optimizer for transformers. Combines the best ideas in optimization." },
    { content: "Complete loop: forward pass ‚Üí compute loss ‚Üí backward pass ‚Üí update parameters ‚Üí zero gradients ‚Üí repeat. 500+ iterations. Loss goes from 3.33 to ~2.0.", highlight: "Training loop = the heartbeat of machine learning. Forward ‚Üí Loss ‚Üí Backward ‚Üí Update ‚Üí Repeat." },
  ],
  inference: [
    { content: "Training: forward + backward + update (parallel, gradients needed). Inference: forward only (sequential, no gradients ‚Üí fast, low memory). Training = studying for an exam, Inference = taking the exam.", highlight: "Training = learning, Inference = applying what was learned. Very different computational profiles." },
    { content: "Generate one token at a time: feed current sequence ‚Üí get probability distribution ‚Üí sample next token ‚Üí append ‚Üí repeat until EOS. Each step requires a full forward pass.", highlight: "Autoregressive = each new token depends on ALL previous tokens." },
    { content: "Temperature < 1: sharper distribution ‚Üí more predictable. Temperature > 1: flatter ‚Üí more creative/random. Top-k: only consider the k most likely tokens, ignore the rest.", highlight: "Temperature controls the creativity-coherence tradeoff." },
    { content: "Without cache: re-compute attention for ALL previous tokens at each step. With KV cache: store K,V from previous steps, only compute for the new token. Huge speedup!", highlight: "KV cache: the key optimization that makes autoregressive generation practical." },
    { content: "Explore all 3,648 parameters: where are they, what do they do? Weight initialization: small random values (Gaussian, œÉ=0.02). Too large ‚Üí explosion, too small ‚Üí vanishing signals.", highlight: "Initialization matters: the right starting point makes training much easier." },
  ],
  evolution: [
    { content: "More parameters = lower loss, but with diminishing returns. Chinchilla scaling: optimal compute allocation between model size and data. Key insight: most models are undertrained!", highlight: "Scaling laws: the empirical foundation of modern AI. Predictable performance from compute budget." },
    { content: "From CPUs to GPUs to TPUs to custom AI chips. GPU parallelism is what makes transformer training feasible. Memory bandwidth is often the bottleneck.", highlight: "Hardware evolution drives AI capability. Each generation enables 10√ó larger models." },
    { content: "Data collection ‚Üí cleaning ‚Üí tokenization ‚Üí training ‚Üí evaluation. Modern models train on trillions of tokens. Data quality matters as much as quantity.", highlight: "Data pipeline: garbage in, garbage out. The most important and least glamorous part of AI." },
    { content: "BPE ‚Üí WordPiece ‚Üí Unigram ‚Üí SentencePiece. Evolution towards: language-agnostic, efficient, robust tokenization. Modern tokenizers handle 100K+ vocabulary.", highlight: "Tokenization has evolved from simple character splits to sophisticated subword algorithms." },
    { content: "Full attention is O(n¬≤). Solutions: sparse attention, linear attention, flash attention. FlashAttention: same result, 2-4√ó faster through memory-aware computation.", highlight: "FlashAttention: the breakthrough that made long-context models practical." },
    { content: "LLaMA, Mistral, Phi, Gemma, Qwen ‚Äî the open source ecosystem is thriving. Open weights enable research, fine-tuning, and deployment without API dependencies.", highlight: "Open source AI: democratizing access to state-of-the-art models." },
    { content: "MoE (Mixture of Experts), multimodal models, agents, reasoning chains, RLHF/DPO alignment. The field is evolving at unprecedented speed.", highlight: "The pace of AI innovation continues to accelerate. Today\'s cutting edge is tomorrow\'s baseline." },
  ],
  paper: [
    { content: "In 2017, Google researchers discarded RNNs and CNNs entirely and built the Transformer model using only attention. Better results AND much faster.", highlight: "Old approach (RNN): processes each word SEQUENTIALLY. Transformer looks at ALL words simultaneously." },
    { content: "RNN is sequential ‚Üí can\'t parallelize ‚Üí slow. Information from early words is lost in long sentences. Gradient explosion/vanishing occurs.", highlight: "RNN: O(n) steps, Transformer: O(1) steps. Everyone sees everyone!" },
    { content: "Query: What am I looking for? Key: What do I have? Value: Here\'s my information. High Q¬∑K ‚Üí take more information from that word\'s Value!", highlight: "Library analogy: Query = topic you\'re searching for, Key = book label, Value = book content." },
    { content: "3 key formulas: ‚ë† Dot Product ‚ë° Softmax ‚ë¢ Scaled Dot-Product Attention. Explore each with sliders.", highlight: "Attention(Q,K,V) = softmax(QK·µÄ/‚àöd)V ‚Äî the paper\'s most famous formula." },
    { content: "Encoder understands the input sentence (6 layers). Decoder generates the output sentence (6 layers). Each layer: Attention + FFN + Residual + LayerNorm.", highlight: "Decoder has causal mask: cannot see future words!" },
    { content: "Transformer has no notion of order! Sin/cos waves add a unique \'fingerprint\' to each position. Different frequencies capture patterns at different scales.", highlight: "Low dimensions change fast (treble), high dimensions change slow (bass) ‚Äî like a piano!" },
    { content: "4.5M sentence pairs, 8√ó P100 GPU, 3.5 days. EN‚ÜíDE: 28.4 BLEU (record!). EN‚ÜíFR: 41.8 BLEU. Warmup + label smoothing + dropout.", highlight: "Base model: 65M parameters. Big model: 213M parameters. Today\'s GPT-4: ~1T+ parameters!" },
    { content: "90K+ citations! GPT, BERT, ViT, DALL-E, AlphaFold, Copilot ‚Äî all Transformer-based. A 15-page paper changed all of AI.", highlight: "Not just language: vision (ViT), protein (AlphaFold), music (MusicGen), code (Copilot)." },
  ],
};


export { EN_CONTENT };
